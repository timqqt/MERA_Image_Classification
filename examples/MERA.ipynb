{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERA\n",
    "MERA structure TN for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_v2_behavior\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(\"/hpc/group/carin/fk43/FanjieKong/Megapixels/new_tfquantum/quantum/TensorNetwork/mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000, 10)\n",
      "(10000, 28, 28, 1) (10000, 10)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype(np.float32)\n",
    "y_train = to_categorical(y_train, 10).astype(np.float32)\n",
    "x_test = x_test.reshape((10000, 28, 28, 1))\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_train = tf.image.resize(x_train, [8,8])\n",
    "xx_test = tf.image.resize(x_test, [8,8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super(TNLayer, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        self.dim = dim\n",
    "        self.a_var = tf.Variable(tf.random.normal(shape=(dim, dim, 2), stddev=1.0/dim), name=\"a\", trainable=True)\n",
    "        self.b_var = tf.Variable(tf.random.normal(shape=(dim, dim, 2), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(dim, dim)), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        print(inputs.shape)\n",
    "        def f(input_vec, a_var, b_var, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            input_vec = tf.reshape(input_vec, (self.dim, self.dim))\n",
    "\n",
    "            # Now we create the network.\n",
    "            a = tn.Node(a_var)\n",
    "            b = tn.Node(b_var)\n",
    "            x_node = tn.Node(input_vec)\n",
    "            a[1] ^ x_node[0] # ^ means connection\n",
    "            b[1] ^ x_node[1]\n",
    "            a[2] ^ b[2]\n",
    "\n",
    "            # The TN should now look like this\n",
    "            #   |     |\n",
    "            #   a --- b\n",
    "            #    \\   /\n",
    "            #      x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            c = a @ x_node\n",
    "            result = (c @ b).tensor\n",
    "\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.a_var, self.b_var, self.bias), inputs)\n",
    "        return tf.nn.relu(tf.reshape(result, (-1, self.dim**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super(TNLayer, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        self.dim = dim\n",
    "        self.a_var = tf.Variable(tf.random.normal(shape=(dim, dim, 2), stddev=1.0/dim), name=\"a\", trainable=True)\n",
    "        self.b_var = tf.Variable(tf.random.normal(shape=(dim, dim, 2), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(dim, dim)), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        print(inputs.shape)\n",
    "        def f(input_vec, a_var, b_var, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            input_vec = tf.reshape(input_vec, (self.dim, self.dim))\n",
    "\n",
    "            # Now we create the network.\n",
    "            a = tn.Node(a_var)\n",
    "            b = tn.Node(b_var)\n",
    "            x_node = tn.Node(input_vec)\n",
    "            a[1] ^ x_node[0] # ^ means connection\n",
    "            b[1] ^ x_node[1]\n",
    "            a[2] ^ b[2]\n",
    "\n",
    "            # The TN should now look like this\n",
    "            #   |     |\n",
    "            #   a --- b\n",
    "            #    \\   /\n",
    "            #      x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            c = a @ x_node\n",
    "            result = (c @ b).tensor\n",
    "\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.a_var, self.b_var, self.bias), inputs)\n",
    "        return tf.nn.relu(tf.reshape(result, (-1, self.dim**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim, n_nodes):\n",
    "        super(MERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        self.dim = dim\n",
    "        self.a_var = tf.Variable(tf.random.normal(shape=(11, 11), stddev=1.0/dim), name=\"a\", trainable=True)\n",
    "        self.b_var = tf.Variable(tf.random.normal(shape=(11, 11, 11), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.c_var = tf.Variable(tf.random.normal(shape=(16, 16), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        \n",
    "        self.d_var = tf.Variable(tf.random.normal(shape=(11, 11, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.e_var = tf.Variable(tf.random.normal(shape=(11, 16, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.f_var = tf.Variable(tf.random.normal(shape=(10, 10, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(10)), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, a_var, b_var, c_var, d_var, e_var, f_var, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            print(input_vec)\n",
    "            input_vec = tf.reshape(input_vec, (11, 11, 16))\n",
    "        \n",
    "            # Now we create the network.\n",
    "            a = tn.Node(a_var)\n",
    "            b = tn.Node(b_var)\n",
    "            c = tn.Node(c_var)\n",
    "            d = tn.Node(d_var)\n",
    "            e = tn.Node(e_var)\n",
    "            f = tn.Node(f_var)\n",
    "            \n",
    "            x_node = tn.Node(input_vec)\n",
    "            a[0] ^ x_node[0]\n",
    "            b[0] ^ x_node[1]\n",
    "            c[0] ^ x_node[2]\n",
    "            a[1] ^ d[0]\n",
    "            b[1] ^ d[1]\n",
    "            b[2] ^ e[0]\n",
    "            c[1] ^ e[1]\n",
    "            d[2] ^ f[0]\n",
    "            e[2] ^ f[1]\n",
    "            \n",
    "\n",
    "            # The TN should now look like this\n",
    "            #          |\n",
    "            #          f\n",
    "            #       /    \\\n",
    "            #     d       e\n",
    "            #   /    \\ /    \\\n",
    "            #   a     b     c\n",
    "            #    \\   |     /\n",
    "            #        x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            # c1 = a @ x_node # avoid trace edge?\n",
    "            # c2 = c1 @ b\n",
    "            #c1 = a @ x_node\n",
    "            #c2 = c1 @ b\n",
    "            #c3 = c2 @ c\n",
    "            #result = c3.tensor\n",
    "            nodes = tn.reachable(f)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            \n",
    "            #result = (c @ b).tensor\n",
    "\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.a_var, self.b_var, self.c_var, \n",
    "                                                 self.d_var, self.e_var, self.f_var, self.bias), inputs)\n",
    "        return tf.nn.relu(tf.reshape(result, (-1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 14, 14, 4), dtype=float32, numpy=\n",
       "array([[[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf.image.extract_patches(images=tf.ones((5, 28, 28, 1)),\n",
    "                           sizes=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           rates=[1, 1, 1, 1],\n",
    "                           padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones((5, 28, 28, 1)).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for e_ones in tf.ones((5, 28, 28, 1)):\n",
    "    print(tf.sin(e_ones[0,0,:]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.807354922057604"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [tn.Node([1,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(7):\n",
    "    for ii in range(7):\n",
    "        test.append(0+ 28 * i+2*ii)\n",
    "        test.append(1+ 28 * i+2*ii)\n",
    "        test.append(14+ 28 * i+2*ii)\n",
    "        test.append(15+ 28 * i+2*ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()\n",
    "class twoDtreeMERA(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, bond_dims, output_dims, n_layers=None):\n",
    "        super(twoDtreeMERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        if n_layers is None:\n",
    "            n_layers = np.floor(np.log2(input_dim))\n",
    "        self.n_layers = n_layers\n",
    "        in_dims = 2\n",
    "        dims = input_dim\n",
    "        self.entanglers = []\n",
    "        self.isometries= []\n",
    "        for i in range(n_layers):\n",
    "            if dims % 2 == 0:\n",
    "                dims = int(dims//2)\n",
    "                self.entanglers.append([tf.Variable(tf.random.normal\n",
    "                                             (shape=(in_dims, in_dims, \n",
    "                                                     in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=1.0/bond_dims), \n",
    "                                              trainable=True) for j in range(dims**2)])\n",
    "                in_dims = bond_dims\n",
    "                self.isometries.append([tf.Variable(tf.random.normal(shape=(bond_dims, bond_dims, bond_dims, \n",
    "                                                                            bond_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/bond_dims),\n",
    "                                            trainable=True) for j in range(dims**2)])\n",
    "#             else:\n",
    "#                 # odd number layer\n",
    "#                 dims = int(dims//2)\n",
    "#                 self.entanglers.append([tf.Variable(tf.random.normal\n",
    "#                                              (shape=(in_dims, in_dims, \n",
    "#                                                      in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "#                                               stddev=1.0/bond_dims), \n",
    "#                                               trainable=True) for j in range(dims**2)]+[tf.Variable(tf.random.normal\n",
    "#                                              (shape=(in_dims, in_dims, \n",
    "#                                                      in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "#                                               stddev=1.0/bond_dims), \n",
    "#                                               trainable=True)])\n",
    "                \n",
    "                \n",
    "                \n",
    "        self.final_mps = tf.Variable(tf.random.normal(shape=(bond_dims,)*(dims**2) + (output_dims,)\n",
    "                                                                 , stddev=1.0/output_dims))\n",
    "\n",
    "        \n",
    "        self.bias = tf.Variable(tf.zeros(shape=(output_dims)), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, entanglers, isometries, final_mps, bias_var, n_layers):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            #print(input_vec)\n",
    "            patches = tf.image.extract_patches(images=tf.reshape(input_vec, \n",
    "                                                       (1, input_vec.shape[0],  input_vec.shape[1],  input_vec.shape[2])),\n",
    "                           sizes=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           rates=[1, 1, 1, 1],\n",
    "                           padding='VALID')\n",
    "            # (1, 14, 14, 4)\n",
    "            # Z trace permute the input patches?\n",
    "            patches = tf.reshape(patches, (patches.shape[1]*patches.shape[2], patches.shape[3]))\n",
    "            # (196, 4)\n",
    "            #print(patches)\n",
    "            patches = tf.unstack(patches)\n",
    "            input_patches = [tf.concat([tf.expand_dims(tf.sin(e_patch), axis=1),\n",
    "                                                 tf.expand_dims(tf.cos(e_patch), axis=1)], axis=1) for e_patch in patches]\n",
    "\n",
    "#             for e_patch in patches:\n",
    "#                 input_patches.append(tf.expand_dims(tf.concat([tf.expand_dims(tf.sin(e_patch), axis=1),\n",
    "#                                                 tf.expand_dims(tf.cos(e_patch), axis=1)], axis=1), axis=0))\n",
    "                \n",
    "#             input_patches = tf.concat(input_patches, axis=0)\n",
    "            # create nodes\n",
    "            input_nodes_list = []\n",
    "            for e_ip in input_patches:\n",
    "                input_nodes_list.append([tn.Node(e_ip[0, :]),\n",
    "                tn.Node(e_ip[1, :]),\n",
    "                tn.Node(e_ip[2, :]),\n",
    "                tn.Node(e_ip[3, :])])\n",
    "            \n",
    "            entangler_nodes_list = []\n",
    "            for layer in range(self.n_layers):\n",
    "                entangler_nodes_list_in_one_layer = []\n",
    "                for idx, e_e in enumerate(entanglers[layer]):\n",
    "                    entangler_nodes_list_in_one_layer.append(tn.Node(e_e))\n",
    "                entangler_nodes_list.append(entangler_nodes_list_in_one_layer)\n",
    "                                     \n",
    "            isometries_nodes_list = []\n",
    "            for layer in range(self.n_layers):\n",
    "                isometries_nodes_list_in_one_layer = []\n",
    "                for idx, e_i in enumerate(isometries[layer]):\n",
    "                    isometries_nodes_list_in_one_layer.append(tn.Node(e_i))\n",
    "                isometries_nodes_list.append(isometries_nodes_list_in_one_layer)\n",
    "            \n",
    "            mps_node = tn.Node(final_mps)\n",
    "            #connect nodes           \n",
    "            for i in range(len(entangler_nodes_list[0])):                 \n",
    "                input_nodes_list[i][0][0] ^ entangler_nodes_list[0][i][0]\n",
    "                input_nodes_list[i][1][0] ^ entangler_nodes_list[0][i][1]\n",
    "                input_nodes_list[i][2][0] ^ entangler_nodes_list[0][i][2]\n",
    "                input_nodes_list[i][3][0] ^ entangler_nodes_list[0][i][3]\n",
    "\n",
    "                isometries_nodes_list[0][i][0] ^ entangler_nodes_list[0][i][4]\n",
    "                isometries_nodes_list[0][i][1] ^ entangler_nodes_list[0][i][5]\n",
    "                isometries_nodes_list[0][i][2] ^ entangler_nodes_list[0][i][6]\n",
    "                isometries_nodes_list[0][i][3] ^ entangler_nodes_list[0][i][7]\n",
    "            #re-permute iso list to keep spatial coordinates\n",
    "            #print(len(isometries_nodes_list[0]))\n",
    "            new_iso_nodes_list = []\n",
    "            for i in range(6):\n",
    "                for ii in range(6):\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][0+ 24 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][1+ 24 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][12+ 24 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][13+ 24 * i+2*ii])\n",
    "            #print(len(new_iso_nodes_list))\n",
    "            isometries_nodes_list[0] = new_iso_nodes_list\n",
    "            ###\n",
    "            #print(len(isometries_nodes_list[0]))\n",
    "            #print(len(entangler_nodes_list[1]))\n",
    "            for i in range(len(entangler_nodes_list[1])):                 \n",
    "                isometries_nodes_list[0][i*4+0][4] ^ entangler_nodes_list[1][i][0]\n",
    "                isometries_nodes_list[0][i*4+1][4] ^ entangler_nodes_list[1][i][1]\n",
    "                isometries_nodes_list[0][i*4+2][4] ^ entangler_nodes_list[1][i][2]\n",
    "                isometries_nodes_list[0][i*4+3][4] ^ entangler_nodes_list[1][i][3]\n",
    "                \n",
    "                isometries_nodes_list[1][i][0] ^ entangler_nodes_list[1][i][4]        \n",
    "                isometries_nodes_list[1][i][1] ^ entangler_nodes_list[1][i][5]        \n",
    "                isometries_nodes_list[1][i][2] ^ entangler_nodes_list[1][i][6]        \n",
    "                isometries_nodes_list[1][i][3] ^ entangler_nodes_list[1][i][7]        \n",
    "            \n",
    "            new_iso_nodes_list = []\n",
    "            for i in range(3):\n",
    "                for ii in range(3):\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[1][0+ 12 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[1][1+ 12 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[1][6+ 12 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[1][7+ 12 * i+2*ii])\n",
    "            isometries_nodes_list[1] = new_iso_nodes_list\n",
    "            \n",
    "            for i in range(len(entangler_nodes_list[2])):                 \n",
    "                isometries_nodes_list[1][i*4+0][4] ^ entangler_nodes_list[2][i][0]\n",
    "                isometries_nodes_list[1][i*4+1][4] ^ entangler_nodes_list[2][i][1]\n",
    "                isometries_nodes_list[1][i*4+2][4] ^ entangler_nodes_list[2][i][2]\n",
    "                isometries_nodes_list[1][i*4+3][4] ^ entangler_nodes_list[2][i][3]\n",
    "                \n",
    "                isometries_nodes_list[2][i][0] ^ entangler_nodes_list[2][i][4]        \n",
    "                isometries_nodes_list[2][i][1] ^ entangler_nodes_list[2][i][5]        \n",
    "                isometries_nodes_list[2][i][2] ^ entangler_nodes_list[2][i][6]        \n",
    "                isometries_nodes_list[2][i][3] ^ entangler_nodes_list[2][i][7] \n",
    "                \n",
    "            \n",
    "                isometries_nodes_list[2][i][4] ^ mps_node[i] \n",
    "            # final mps connection\n",
    "                                                 \n",
    "\n",
    "            # The TN should now look like this\n",
    "            #          |\n",
    "            #          f\n",
    "            #       /    \\\n",
    "            #     d       e\n",
    "            #   /    \\ /    \\\n",
    "            #   a     b     c\n",
    "            #    \\   |     /\n",
    "            #        x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            # c1 = a @ x_node # avoid trace edge?\n",
    "            # c2 = c1 @ b\n",
    "            #c1 = a @ x_node\n",
    "            #c2 = c1 @ b\n",
    "            #c3 = c2 @ c\n",
    "            #result = c3.tensor\n",
    "            nodes = tn.reachable(mps_node)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            #result = (c @ b).tensor\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.entanglers, \n",
    "                                                 self.isometries, self.final_mps, self.bias, self.n_layers), inputs)\n",
    "        return tf.nn.relu(tf.reshape(result, (-1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, bond_dims, output_dims, n_layers=None):\n",
    "        super(MERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        if n_layers is None:\n",
    "            n_layers = np.floor(np.log2(input_dim))\n",
    "        self.n_layers = n_layers\n",
    "        in_dims = 2\n",
    "        dims = input_dim\n",
    "        self.entanglers = []\n",
    "        self.isometries= []\n",
    "        for i in range(n_layers):\n",
    "            if dims % 2 == 0:\n",
    "                dims = int(dims//2)\n",
    "                self.entanglers.append([tf.Variable(tf.random.normal\n",
    "                                             (shape=(in_dims, in_dims, \n",
    "                                                     in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=1.0/bond_dims), \n",
    "                                              trainable=True) for j in range(dims**2)])\n",
    "                in_dims = bond_dims\n",
    "                self.isometries.append([tf.Variable(tf.random.normal(shape=(bond_dims, bond_dims, bond_dims, \n",
    "                                                                            bond_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/bond_dims),\n",
    "                                            trainable=True) for j in range(dims**2)])\n",
    "#             else:\n",
    "#                 # odd number layer\n",
    "#                 dims = int(dims//2)\n",
    "#                 self.entanglers.append([tf.Variable(tf.random.normal\n",
    "#                                              (shape=(in_dims, in_dims, \n",
    "#                                                      in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "#                                               stddev=1.0/bond_dims), \n",
    "#                                               trainable=True) for j in range(dims**2)]+[tf.Variable(tf.random.normal\n",
    "#                                              (shape=(in_dims, in_dims, \n",
    "#                                                      in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "#                                               stddev=1.0/bond_dims), \n",
    "#                                               trainable=True)])\n",
    "                \n",
    "                \n",
    "                \n",
    "        self.final_mps = tf.Variable(tf.random.normal(shape=(bond_dims,)*(dims**2) + (output_dims,)\n",
    "                                                                 , stddev=1.0/output_dims))\n",
    "\n",
    "       # print( self.final_mps)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(output_dims,)), name=\"bias\", trainable=True)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, entanglers, isometries, final_mps, bias_var, n_layers):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            #print(input_vec)\n",
    "            patches = tf.image.extract_patches(images=tf.reshape(input_vec, \n",
    "                                                       (1, input_vec.shape[0],  input_vec.shape[1],  input_vec.shape[2])),\n",
    "                           sizes=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           rates=[1, 1, 1, 1],\n",
    "                           padding='VALID')\n",
    "            # (1, 14, 14, 4)\n",
    "            # Z trace permute the input patches?\n",
    "            patches = tf.reshape(patches, (patches.shape[1]*patches.shape[2], patches.shape[3]))\n",
    "            # (196, 4)\n",
    "            #print(patches)\n",
    "            patches = tf.unstack(patches)\n",
    "            input_patches = [tf.concat([tf.expand_dims(tf.sin(e_patch), axis=1),\n",
    "                                                 tf.expand_dims(tf.cos(e_patch), axis=1)], axis=1) for e_patch in patches]\n",
    "\n",
    "#             for e_patch in patches:\n",
    "#                 input_patches.append(tf.expand_dims(tf.concat([tf.expand_dims(tf.sin(e_patch), axis=1),\n",
    "#                                                 tf.expand_dims(tf.cos(e_patch), axis=1)], axis=1), axis=0))\n",
    "                \n",
    "#             input_patches = tf.concat(input_patches, axis=0)\n",
    "            # create nodes\n",
    "            input_nodes_list = []\n",
    "            for e_ip in input_patches:\n",
    "                input_nodes_list.append([tn.Node(e_ip[0, :]),\n",
    "                tn.Node(e_ip[1, :]),\n",
    "                tn.Node(e_ip[2, :]),\n",
    "                tn.Node(e_ip[3, :])])\n",
    "            \n",
    "            entangler_nodes_list = []\n",
    "            for layer in range(self.n_layers):\n",
    "                entangler_nodes_list_in_one_layer = []\n",
    "                for idx, e_e in enumerate(entanglers[layer]):\n",
    "                    entangler_nodes_list_in_one_layer.append(tn.Node(e_e))\n",
    "                entangler_nodes_list.append(entangler_nodes_list_in_one_layer)\n",
    "                                     \n",
    "            isometries_nodes_list = []\n",
    "            for layer in range(self.n_layers):\n",
    "                isometries_nodes_list_in_one_layer = []\n",
    "                for idx, e_i in enumerate(isometries[layer]):\n",
    "                    isometries_nodes_list_in_one_layer.append(tn.Node(e_i))\n",
    "                isometries_nodes_list.append(isometries_nodes_list_in_one_layer)\n",
    "            \n",
    "            mps_node = tn.Node(final_mps)\n",
    "            #connect nodes           \n",
    "            for i in range(len(entangler_nodes_list[0])):                 \n",
    "                input_nodes_list[i][0][0] ^ entangler_nodes_list[0][i][0]\n",
    "                input_nodes_list[i][1][0] ^ entangler_nodes_list[0][i][1]\n",
    "                input_nodes_list[i][2][0] ^ entangler_nodes_list[0][i][2]\n",
    "                input_nodes_list[i][3][0] ^ entangler_nodes_list[0][i][3]\n",
    "\n",
    "                isometries_nodes_list[0][i][0] ^ entangler_nodes_list[0][i][4]\n",
    "                isometries_nodes_list[0][i][1] ^ entangler_nodes_list[0][i][5]\n",
    "                isometries_nodes_list[0][i][2] ^ entangler_nodes_list[0][i][6]\n",
    "                isometries_nodes_list[0][i][3] ^ entangler_nodes_list[0][i][7]\n",
    "            #re-permute iso list to keep spatial coordinates\n",
    "            #print(len(isometries_nodes_list[0]))\n",
    "            new_iso_nodes_list = []\n",
    "            for i in range(6):\n",
    "                for ii in range(6):\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][0+ 24 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][1+ 24 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][12+ 24 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][13+ 24 * i+2*ii])\n",
    "            #print(len(new_iso_nodes_list))\n",
    "            isometries_nodes_list[0] = new_iso_nodes_list\n",
    "            ###\n",
    "            #print(len(isometries_nodes_list[0]))\n",
    "            #print(len(entangler_nodes_list[1]))\n",
    "            for i in range(len(entangler_nodes_list[1])):                 \n",
    "                isometries_nodes_list[0][i*4+0][4] ^ entangler_nodes_list[1][i][0]\n",
    "                isometries_nodes_list[0][i*4+1][4] ^ entangler_nodes_list[1][i][1]\n",
    "                isometries_nodes_list[0][i*4+2][4] ^ entangler_nodes_list[1][i][2]\n",
    "                isometries_nodes_list[0][i*4+3][4] ^ entangler_nodes_list[1][i][3]\n",
    "                \n",
    "                isometries_nodes_list[1][i][0] ^ entangler_nodes_list[1][i][4]        \n",
    "                isometries_nodes_list[1][i][1] ^ entangler_nodes_list[1][i][5]        \n",
    "                isometries_nodes_list[1][i][2] ^ entangler_nodes_list[1][i][6]        \n",
    "                isometries_nodes_list[1][i][3] ^ entangler_nodes_list[1][i][7]        \n",
    "            \n",
    "            new_iso_nodes_list = []\n",
    "            for i in range(3):\n",
    "                for ii in range(3):\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[1][0+ 12 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[1][1+ 12 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[1][6+ 12 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[1][7+ 12 * i+2*ii])\n",
    "            isometries_nodes_list[1] = new_iso_nodes_list\n",
    "            \n",
    "            for i in range(len(entangler_nodes_list[2])):                 \n",
    "                isometries_nodes_list[1][i*4+0][4] ^ entangler_nodes_list[2][i][0]\n",
    "                isometries_nodes_list[1][i*4+1][4] ^ entangler_nodes_list[2][i][1]\n",
    "                isometries_nodes_list[1][i*4+2][4] ^ entangler_nodes_list[2][i][2]\n",
    "                isometries_nodes_list[1][i*4+3][4] ^ entangler_nodes_list[2][i][3]\n",
    "                \n",
    "                isometries_nodes_list[2][i][0] ^ entangler_nodes_list[2][i][4]        \n",
    "                isometries_nodes_list[2][i][1] ^ entangler_nodes_list[2][i][5]        \n",
    "                isometries_nodes_list[2][i][2] ^ entangler_nodes_list[2][i][6]        \n",
    "                isometries_nodes_list[2][i][3] ^ entangler_nodes_list[2][i][7] \n",
    "                \n",
    "            \n",
    "                isometries_nodes_list[2][i][4] ^ mps_node[i] \n",
    "            # final mps connection\n",
    "                                                 \n",
    "\n",
    "        \n",
    "            nodes = tn.reachable(mps_node)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            print(result)\n",
    "            #result = (c @ b).tensor\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        output = tf.vectorized_map(lambda vec: f(vec, self.entanglers, \n",
    "                                                 self.isometries, self.final_mps, self.bias, self.n_layers), inputs)\n",
    "        return tf.nn.relu(tf.reshape(output, (-1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, bond_dims, output_dims, n_layers=None):\n",
    "        super(MERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        if n_layers is None:\n",
    "            n_layers = np.floor(np.log2(input_dim))\n",
    "        self.n_layers = n_layers\n",
    "        in_dims = 1\n",
    "        dims = input_dim\n",
    "        self.entanglers = []\n",
    "        self.isometries= []\n",
    "        for i in range(n_layers):\n",
    "            if dims % 2 == 0:\n",
    "                dims = int(dims//2)\n",
    "                self.entanglers.append([tf.Variable(tf.random.normal\n",
    "                                             (shape=(in_dims, in_dims, \n",
    "                                                     in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=1.0/bond_dims), \n",
    "                                              trainable=True) for j in range(dims**2)])\n",
    "                in_dims = bond_dims\n",
    "                self.isometries.append([tf.Variable(tf.random.normal(shape=(bond_dims, bond_dims, bond_dims, \n",
    "                                                                            bond_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/bond_dims),\n",
    "                                            trainable=True) for j in range(dims**2)])\n",
    "#             else:\n",
    "#                 # odd number layer\n",
    "#                 dims = int(dims//2)\n",
    "#                 self.entanglers.append([tf.Variable(tf.random.normal\n",
    "#                                              (shape=(in_dims, in_dims, \n",
    "#                                                      in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "#                                               stddev=1.0/bond_dims), \n",
    "#                                               trainable=True) for j in range(dims**2)]+[tf.Variable(tf.random.normal\n",
    "#                                              (shape=(in_dims, in_dims, \n",
    "#                                                      in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "#                                               stddev=1.0/bond_dims), \n",
    "#                                               trainable=True)])\n",
    "                \n",
    "                \n",
    "                \n",
    "        self.final_mps = tf.Variable(tf.random.normal(shape=(bond_dims,)*(dims**2) + (output_dims,)\n",
    "                                                                 , stddev=1.0/output_dims))\n",
    "\n",
    "        \n",
    "        self.bias = tf.Variable(tf.zeros(shape=(output_dims,)), name=\"bias\", trainable=True)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, entanglers, isometries, final_mps, bias_var, n_layers):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            print(input_vec)\n",
    "            #input_vec = tf.ones((8, 8, 1))\n",
    "            #print(input_vec)\n",
    "            patches = tf.image.extract_patches(images=tf.reshape(input_vec, \n",
    "                                                       (1, input_vec.shape[0],  input_vec.shape[1],  input_vec.shape[2])),\n",
    "                           sizes=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           rates=[1, 1, 1, 1],\n",
    "                           padding='VALID')\n",
    "            # (1, 14, 14, 4)\n",
    "            # Z trace permute the input patches?\n",
    "            patches = tf.reshape(patches, (patches.shape[1]*patches.shape[2], patches.shape[3]))\n",
    "            # (196, 4)\n",
    "            #print(patches)\n",
    "            #input_patches = tf.concat([tf.expand_dims(tf.sin(patches), axis=2),tf.expand_dims(tf.cos(patches), axis=2)], axis=2)\n",
    "#             patches = tf.unstack(patches)\n",
    "#             input_patches = [tf.concat([tf.expand_dims(tf.sin(e_patch), axis=1),\n",
    "#                                                  tf.expand_dims(tf.cos(e_patch), axis=1)], axis=1) for e_patch in patches]\n",
    "            #input_patches = [tf.ones((4,2)) for e_patch in patches]\n",
    "            # create nodes\n",
    "            #print(input_patches)\n",
    "            #input_patches = tf.ones((16, 4, 2))\n",
    "            input_patches = tf.expand_dims(patches, axis=2)\n",
    "            input_nodes_list = []\n",
    "            for i in range(input_patches.shape[0]):\n",
    "                input_nodes_list.append([tn.Node(input_patches[i, 0, :]),\n",
    "                tn.Node(input_patches[i,1, :]),\n",
    "                tn.Node(input_patches[i,2, :]),\n",
    "                tn.Node(input_patches[i,3, :])])\n",
    "                \n",
    "                \n",
    "            print(len(input_nodes_list))\n",
    "            entangler_nodes_list = []\n",
    "            for layer in range(self.n_layers):\n",
    "                entangler_nodes_list_in_one_layer = []\n",
    "                for idx, e_e in enumerate(entanglers[layer]):\n",
    "                    entangler_nodes_list_in_one_layer.append(tn.Node(e_e))\n",
    "                entangler_nodes_list.append(entangler_nodes_list_in_one_layer)\n",
    "                                     \n",
    "            isometries_nodes_list = []\n",
    "            for layer in range(self.n_layers):\n",
    "                isometries_nodes_list_in_one_layer = []\n",
    "                for idx, e_i in enumerate(isometries[layer]):\n",
    "                    isometries_nodes_list_in_one_layer.append(tn.Node(e_i))\n",
    "                isometries_nodes_list.append(isometries_nodes_list_in_one_layer)\n",
    "            \n",
    "            mps_node = tn.Node(final_mps)\n",
    "            #print(mps_node[0])\n",
    "            #print(input_nodes_list[0][0])\n",
    "            #connect nodes           \n",
    "            for i in range(len(entangler_nodes_list[0])):                 \n",
    "                input_nodes_list[i][0][0] ^ entangler_nodes_list[0][i][0]\n",
    "                input_nodes_list[i][1][0] ^ entangler_nodes_list[0][i][1]\n",
    "                input_nodes_list[i][2][0] ^ entangler_nodes_list[0][i][2]\n",
    "                input_nodes_list[i][3][0] ^ entangler_nodes_list[0][i][3]\n",
    "\n",
    "                isometries_nodes_list[0][i][0] ^ entangler_nodes_list[0][i][4]\n",
    "                isometries_nodes_list[0][i][1] ^ entangler_nodes_list[0][i][5]\n",
    "                isometries_nodes_list[0][i][2] ^ entangler_nodes_list[0][i][6]\n",
    "                isometries_nodes_list[0][i][3] ^ entangler_nodes_list[0][i][7]\n",
    "            #re-permute iso list to keep spatial coordinates\n",
    "            #print(len(isometries_nodes_list[0]))\n",
    "            new_iso_nodes_list = []\n",
    "            for i in range(2):\n",
    "                for ii in range(2):\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][0+ 8 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][1+ 8 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][4+ 8 * i+2*ii])\n",
    "                    new_iso_nodes_list.append(isometries_nodes_list[0][5+ 8 * i+2*ii])\n",
    "            #print(len(new_iso_nodes_list))\n",
    "            isometries_nodes_list[0] = new_iso_nodes_list\n",
    "            ###\n",
    "            #print(len(isometries_nodes_list[0]))\n",
    "            #print(len(entangler_nodes_list[1]))\n",
    "            for i in range(len(entangler_nodes_list[1])):                 \n",
    "                isometries_nodes_list[0][i*4+0][4] ^ entangler_nodes_list[1][i][0]\n",
    "                isometries_nodes_list[0][i*4+1][4] ^ entangler_nodes_list[1][i][1]\n",
    "                isometries_nodes_list[0][i*4+2][4] ^ entangler_nodes_list[1][i][2]\n",
    "                isometries_nodes_list[0][i*4+3][4] ^ entangler_nodes_list[1][i][3]\n",
    "                \n",
    "                isometries_nodes_list[1][i][0] ^ entangler_nodes_list[1][i][4]        \n",
    "                isometries_nodes_list[1][i][1] ^ entangler_nodes_list[1][i][5]        \n",
    "                isometries_nodes_list[1][i][2] ^ entangler_nodes_list[1][i][6]        \n",
    "                isometries_nodes_list[1][i][3] ^ entangler_nodes_list[1][i][7]        \n",
    "                \n",
    "                isometries_nodes_list[1][i][4] ^ mps_node[i] \n",
    "            # final mps connection\n",
    "                                                 \n",
    "\n",
    "        \n",
    "            nodes = tn.reachable(mps_node)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            print(result)\n",
    "            #result = (c @ b).tensor\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        output = tf.vectorized_map(lambda vec: f(vec, self.entanglers, \n",
    "                                                 self.isometries, self.final_mps, self.bias, self.n_layers), inputs)\n",
    "        return tf.nn.relu(tf.reshape(output, (-1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim, n_nodes):\n",
    "        super(MERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 784), we factorize it into a tensor (, 7, 7, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        self.dim = dim\n",
    "        self.a_var = tf.Variable(tf.random.normal(shape=(7, 7), stddev=1.0/dim), name=\"a\", trainable=True)\n",
    "        self.b_var = tf.Variable(tf.random.normal(shape=(7, 7, 7), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.c_var = tf.Variable(tf.random.normal(shape=(16, 16), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        \n",
    "        self.d_var = tf.Variable(tf.random.normal(shape=(7, 7, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.e_var = tf.Variable(tf.random.normal(shape=(7, 16, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.f_var = tf.Variable(tf.random.normal(shape=(10, 10, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(10)), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, a_var, b_var, c_var, d_var, e_var, f_var, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            input_vec = tf.reshape(input_vec, (7, 7, 16))\n",
    "        \n",
    "            # Now we create the network.\n",
    "            a = tn.Node(a_var)\n",
    "            b = tn.Node(b_var)\n",
    "            c = tn.Node(c_var)\n",
    "            d = tn.Node(d_var)\n",
    "            e = tn.Node(e_var)\n",
    "            f = tn.Node(f_var)\n",
    "            \n",
    "            x_node = tn.Node(input_vec)\n",
    "            a[0] ^ x_node[0]\n",
    "            b[0] ^ x_node[1]\n",
    "            c[0] ^ x_node[2]\n",
    "            a[1] ^ d[0]\n",
    "            b[1] ^ d[1]\n",
    "            b[2] ^ e[0]\n",
    "            c[1] ^ e[1]\n",
    "            d[2] ^ f[0]\n",
    "            e[2] ^ f[1]\n",
    "            \n",
    "\n",
    "            # The TN should now look like this\n",
    "            #          |\n",
    "            #          f\n",
    "            #       /    \\\n",
    "            #     d       e\n",
    "            #   /    \\ /    \\\n",
    "            #   a     b     c\n",
    "            #    \\   |     /\n",
    "            #        x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            # c1 = a @ x_node # avoid trace edge?\n",
    "            # c2 = c1 @ b\n",
    "            #c1 = a @ x_node\n",
    "            #c2 = c1 @ b\n",
    "            #c3 = c2 @ c\n",
    "            #result = c3.tensor\n",
    "            nodes = tn.reachable(f)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            \n",
    "            #result = (c @ b).tensor\n",
    "\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.a_var, self.b_var, self.c_var, \n",
    "                                                 self.d_var, self.e_var, self.f_var, self.bias), inputs)\n",
    "        return tf.nn.relu(tf.reshape(result, (-1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mera (MERA)                  multiple                  3268      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 3,268\n",
      "Trainable params: 3,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MERA_model = tf.keras.Sequential()\n",
    "#MERA_model.add(tf.keras.Input(shape=(8,8,1)))\n",
    "MERA_model.add(MERA(10,2))\n",
    "MERA_model.add(tf.keras.layers.Softmax())\n",
    "MERA_model.build(input_shape=(None, 28, 28, 1))\n",
    "MERA_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4408 - accuracy: 0.8719\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3317 - accuracy: 0.9061\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3159 - accuracy: 0.9101\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3095 - accuracy: 0.9120\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3032 - accuracy: 0.9132\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3001 - accuracy: 0.9151\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2961 - accuracy: 0.9157\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.2957 - accuracy: 0.9162\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.2923 - accuracy: 0.9165\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2908 - accuracy: 0.9181\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2910 - accuracy: 0.9179\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2884 - accuracy: 0.9182\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2875 - accuracy: 0.9178\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2875 - accuracy: 0.9179\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2851 - accuracy: 0.9182\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2851 - accuracy: 0.9196\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.2839 - accuracy: 0.9200\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2835 - accuracy: 0.9186\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2809 - accuracy: 0.9193\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2821 - accuracy: 0.9186\n",
      "CPU times: user 1min 16s, sys: 1.5 s, total: 1min 17s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TensorNetwork model\n",
    "MERA_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "tn_hist = MERA_model.fit(x_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "StagingError",
     "evalue": "in converted code:\n\n    <ipython-input-78-65504d2d635d>:119 f  *\n        new_iso_nodes_list.append(isometries_nodes_list[0][12+ 24 * i+2*ii])\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:394 vectorized_map\n        return pfor(loop_fn, batch_size)\n    /tmp/tmp1wuiqs50.py:131 f\n        ag__.for_stmt(ag__.converted_call(range, (6,), None, fscope_1), None, loop_body_7, get_state_7, set_state_7, (), (), ())\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:339 for_stmt\n        return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:350 _py_for_stmt\n        state = body(target, *state)\n    /tmp/tmp1wuiqs50.py:129 loop_body_7\n        ag__.for_stmt(ag__.converted_call(range, (6,), None, fscope_1), None, loop_body_6, get_state_6, set_state_6, (), (), ())\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:339 for_stmt\n        return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:350 _py_for_stmt\n        state = body(target, *state)\n    /tmp/tmp1wuiqs50.py:126 loop_body_6\n        ag__.converted_call(new_iso_nodes_list.append, (isometries_nodes_list[0][12 + 24 * i + 2 * ii],), None, fscope_1)\n\n    IndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-df8f34d6d39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMERA_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMERA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mMERA_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mMERA_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mMERA_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    680\u001b[0m                            'method accepts an `inputs` argument.')\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m           raise ValueError('You cannot build your model by calling `build` '\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStagingError\u001b[0m: in converted code:\n\n    <ipython-input-78-65504d2d635d>:119 f  *\n        new_iso_nodes_list.append(isometries_nodes_list[0][12+ 24 * i+2*ii])\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:394 vectorized_map\n        return pfor(loop_fn, batch_size)\n    /tmp/tmp1wuiqs50.py:131 f\n        ag__.for_stmt(ag__.converted_call(range, (6,), None, fscope_1), None, loop_body_7, get_state_7, set_state_7, (), (), ())\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:339 for_stmt\n        return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:350 _py_for_stmt\n        state = body(target, *state)\n    /tmp/tmp1wuiqs50.py:129 loop_body_7\n        ag__.for_stmt(ag__.converted_call(range, (6,), None, fscope_1), None, loop_body_6, get_state_6, set_state_6, (), (), ())\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:339 for_stmt\n        return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)\n    /hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:350 _py_for_stmt\n        state = body(target, *state)\n    /tmp/tmp1wuiqs50.py:126 loop_body_6\n        ag__.converted_call(new_iso_nodes_list.append, (isometries_nodes_list[0][12 + 24 * i + 2 * ii],), None, fscope_1)\n\n    IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "MERA_model = tf.keras.Sequential()\n",
    "#MERA_model.add(tf.keras.Input(shape=(8,8,1)))\n",
    "MERA_model.add(MERA(input_dim=8, bond_dims=2, output_dims=10, n_layers=2))\n",
    "MERA_model.add(tf.keras.layers.Softmax())\n",
    "MERA_model.build(input_shape=(None, 8, 8, 1))\n",
    "MERA_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mera/loop_body/GatherV2:0\", shape=(1936,), dtype=float32)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 22, 22, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1936)              0         \n",
      "_________________________________________________________________\n",
      "mera (MERA)                  (None, 10)                5688      \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 15,222\n",
      "Trainable params: 15,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Dense = tf.keras.layers.Dense\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
    "\n",
    "tn_model = tf.keras.Sequential()\n",
    "tn_model.add(Conv2D(16, kernel_size=(3,3), activation='relu', name='conv1', input_shape=(28,28,1)))\n",
    "tn_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', name='conv2'))\n",
    "tn_model.add(Conv2D(16, kernel_size=(3,3), activation='relu', name='conv3'))\n",
    "tn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "tn_model.add(tf.keras.layers.Flatten())\n",
    "tn_model.add(MERA(44, 3))\n",
    "tn_model.add(Dense(10, activation='softmax', name='fc2'))\n",
    "tn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "Tensor(\"sequential_5/mera/loop_body/GatherV2:0\", shape=(1936,), dtype=float32)\n",
      "Tensor(\"sequential_5/mera/loop_body/GatherV2:0\", shape=(1936,), dtype=float32)\n",
      " 9792/60000 [===>..........................] - ETA: 1:47 - loss: 1.0357 - accuracy: 0.6535"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TensorNetwork model\n",
    "tn_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "tn_hist = tn_model.fit(x_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"sequential_5/mera/loop_body/GatherV2:0\", shape=(1936,), dtype=float32)\n",
      "10000/10000 [==============================] - 6s 559us/sample - loss: 0.3608 - accuracy: 0.8954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3608161346077919, 0.8954]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TN model\n",
    "tn_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loss = hist.history['loss']\n",
    "#acc = hist.history['accuracy']\n",
    "tn_loss = tn_hist.history['loss']\n",
    "tn_acc = tn_hist.history['accuracy']\n",
    "\n",
    "#plt.plot(loss, label='traditional')\n",
    "plt.plot(tn_loss, label='tn')\n",
    "plt.title('Loss comparsion in training')\n",
    "plt.ylim(0, 0.15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plt.plot(acc, label='traditional')\n",
    "plt.plot(tn_acc, label='tn')\n",
    "plt.title('Accuracy comparsion in training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
