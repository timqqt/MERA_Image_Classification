{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import pandas as pd\n",
    "import os \n",
    "tf.compat.v1.enable_v2_behavior\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"tensorflow\")\n",
    "batch_size = 64\n",
    "class ClutteredMNISTDataset(Sequence):\n",
    "    reg_dataset_size = 11276\n",
    "\n",
    "    def __init__(self, base_path, csv_path, data_scaling=1., num_examples=None, balance=None, num_classes=2):\n",
    "        self.base_path = base_path\n",
    "        self.csv_path = csv_path\n",
    "        self.csv = pd.read_csv(csv_path)\n",
    "        self.data_scaling = data_scaling\n",
    "        self.num_examples = num_examples\n",
    "        self.balance = balance\n",
    "        self.num_classes = num_classes\n",
    "        self.img_paths = self.csv['img_path'].values\n",
    "        self.lbls = self.csv['label'].values.astype(np.int32)\n",
    "        self.weights = np.ones([len(self.img_paths), ])\n",
    "\n",
    "        if self.num_examples is not None and self.balance is not None:\n",
    "            # only rebalance if examples and balance is given\n",
    "            assert self.num_examples <= len(self.img_paths), \\\n",
    "                'not enough examples in dataset {} - {}'.format(self.num_examples, len(self.img_paths))\n",
    "\n",
    "            pos_num = int(self.balance * self.num_examples)\n",
    "            neg_num = self.num_examples - pos_num\n",
    "\n",
    "            pos_mask = (self.lbls == 1)\n",
    "            pos_paths = self.img_paths[pos_mask][:pos_num]\n",
    "\n",
    "            neg_mask = (self.lbls == 0)\n",
    "            neg_paths = self.img_paths[neg_mask][:neg_num]\n",
    "\n",
    "            self.img_paths = np.concatenate([pos_paths, neg_paths], 0)\n",
    "            self.lbls = np.concatenate([np.ones([pos_num, ]), np.zeros([neg_num, ])], 0)\n",
    "            self.weights = np.ones([self.num_examples, ])\n",
    "            self.weights[:pos_num] /= pos_num\n",
    "            self.weights[pos_num:] /= neg_num\n",
    "\n",
    "        self.shrinkage = self.reg_dataset_size // len(self.img_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = os.path.join(self.base_path, self.img_paths[index])\n",
    "        lbl = self.lbls[index].astype(np.int64)\n",
    "        lbl = np.eye(self.num_classes)[lbl].astype(np.int64)\n",
    "        img = np.load(path)\n",
    "        if isinstance(img, np.lib.npyio.NpzFile):\n",
    "            img = img['arr_0']\n",
    "\n",
    "        if self.data_scaling != 1.:\n",
    "            img = zoom(img, self.data_scaling)\n",
    "            img = img.clip(0., 1.)\n",
    "\n",
    "        img = img[:,:,np.newaxis].astype(np.float32)\n",
    "\n",
    "        return img, lbl\n",
    "    \n",
    "\n",
    "class Batcher(Sequence):\n",
    "    \"\"\"Assemble a sequence of things into a sequence of batches.\"\"\"\n",
    "    def __init__(self, sequence, batch_size=16):\n",
    "        self._batch_size = batch_size\n",
    "        self._sequence = sequence\n",
    "        self._idxs = np.arange(len(self._sequence))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self._sequence) / self._batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        start = i*self._batch_size\n",
    "        end = min(len(self._sequence), start+self._batch_size)\n",
    "        data = [self._sequence[j] for j in self._idxs[start:end]]\n",
    "        inputs = [d[0] for d in data]\n",
    "        outputs = [d[1] for d in data]\n",
    "\n",
    "        return self._stack(inputs), self._stack(outputs)\n",
    "\n",
    "    def _stack(self, data):\n",
    "        if data is None:\n",
    "            return None\n",
    "\n",
    "        if not isinstance(data[0], (list, tuple)):\n",
    "            return np.stack(data)\n",
    "\n",
    "        seq = type(data[0])\n",
    "        K = len(data[0])\n",
    "        data = seq(\n",
    "            np.stack([d[k] for d in data])\n",
    "            for k in range(K)\n",
    "        )\n",
    "\n",
    "        return data\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self._idxs)\n",
    "        self._sequence.on_epoch_end()\n",
    "        \n",
    "        \n",
    "        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_v2_behavior\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tn.set_default_backend(\"tensorflow\")\n",
    "\n",
    "data_path = '/datacommons/carin/fk43/NeedleinHaystack/mnist64/'\n",
    "train_data=ClutteredMNISTDataset(base_path=data_path, csv_path=os.path.join(data_path, 'train.csv'), data_scaling=1., num_examples=3000, balance=0.5)\n",
    "val_data = ClutteredMNISTDataset(base_path=data_path, csv_path=os.path.join(data_path, 'val.csv'), data_scaling=1., num_examples=1000, balance=0.5)\n",
    "test_data = ClutteredMNISTDataset(base_path=data_path, csv_path=os.path.join(data_path, 'test.csv'), data_scaling=1., num_examples=1000, balance=0.5)\n",
    "\n",
    "\n",
    "train_data = Batcher(train_data, batch_size=batch_size)\n",
    "val_data = Batcher(val_data, batch_size=batch_size)\n",
    "test_data = Batcher(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "class GridMERAin16(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, kernel_dims, bond_dims, output_dims):\n",
    "        super(GridMERAin16, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        in_dims = int((kernel_dims//4)**2)\n",
    "        self.entanglers = []\n",
    "        self.isometries= []\n",
    "        self.kernel_dims = kernel_dims\n",
    "        self.output_dims = output_dims\n",
    "        #entanglers\n",
    "        self.entanglers1 = tf.Variable(tf.random.normal\n",
    "                                             (shape=(in_dims, in_dims, \n",
    "                                                     in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=1.0/1000000), \n",
    "                                              trainable=True)\n",
    "        self.entanglers2 = tf.Variable(tf.random.normal\n",
    "                                             (shape=(bond_dims, bond_dims, \n",
    "                                                     bond_dims, bond_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=1.0/1000000), \n",
    "                                              trainable=True)\n",
    "        # isometries\n",
    "        self.isometries1 = [tf.Variable(tf.random.normal(shape=(in_dims, in_dims, in_dims, \n",
    "                                                                            bond_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/1000000),\n",
    "                                            trainable=True), \n",
    "                           tf.Variable(tf.random.normal(shape=(in_dims, in_dims, bond_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/1000000),\n",
    "                                            trainable=True),\n",
    "                           tf.Variable(tf.random.normal(shape=(in_dims, bond_dims, in_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/1000000),\n",
    "                                            trainable=True),\n",
    "                           tf.Variable(tf.random.normal(shape=(bond_dims, in_dims, in_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/1000000),\n",
    "                                            trainable=True)]\n",
    "        \n",
    "        self.isometries2 = tf.Variable(tf.random.normal(shape=(bond_dims, bond_dims, bond_dims, \n",
    "                                                                            bond_dims, output_dims)\n",
    "                                                                     , stddev=1.0/1000000),\n",
    "                                            trainable=True)\n",
    "\n",
    "        #print(self.final_mps.shape)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(output_dims,)), name=\"bias\", trainable=True)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, entanglers1, entanglers2, isometries1, isometries2, bias_var, kernel_dims):\n",
    "            input_vv = []\n",
    "            step = int(kernel_dims//4)\n",
    "            for i in range(4):\n",
    "                for ii in range(4):\n",
    "                    input_vv.append(tf.reshape(input_vec[i*step:i*step+step, ii*step:ii*step+step, 0], (1, step**2)))\n",
    "            input_vec = tf.concat(input_vv, axis=0)\n",
    "            input_vec = tf.reshape(input_vec, (16, step**2))\n",
    "            input_vec = tf.unstack(input_vec)\n",
    "            input_nodes = []\n",
    "            for e_iv in input_vec:\n",
    "                input_nodes.append(tn.Node(e_iv))\n",
    "            \n",
    "            e_nodes1 = tn.Node(entanglers1)\n",
    "            e_nodes2 = tn.Node(entanglers2)\n",
    "                \n",
    "                                     \n",
    "            isometries_nodes1 = []\n",
    "            for eiso in isometries1:\n",
    "                isometries_nodes1.append(tn.Node(eiso))\n",
    "            isometries_nodes2 = tn.Node(isometries2)\n",
    "            \n",
    "            \n",
    "            e_nodes1[0] ^ input_nodes[5][0]\n",
    "            e_nodes1[1] ^ input_nodes[6][0]\n",
    "            e_nodes1[2] ^ input_nodes[9][0]\n",
    "            e_nodes1[3] ^ input_nodes[10][0]\n",
    "\n",
    "            e_nodes1[4] ^ isometries_nodes1[0][3]\n",
    "            e_nodes1[5] ^ isometries_nodes1[1][2]\n",
    "            e_nodes1[6] ^ isometries_nodes1[2][1]\n",
    "            e_nodes1[7] ^ isometries_nodes1[3][0]     \n",
    "            \n",
    "            input_nodes[0][0] ^ isometries_nodes1[0][0]\n",
    "            input_nodes[1][0] ^ isometries_nodes1[0][1]\n",
    "            input_nodes[4][0] ^ isometries_nodes1[0][2]\n",
    "            \n",
    "            input_nodes[2][0] ^ isometries_nodes1[1][0]\n",
    "            input_nodes[3][0] ^ isometries_nodes1[1][1]\n",
    "            input_nodes[7][0] ^ isometries_nodes1[1][3]\n",
    "            \n",
    "            input_nodes[8][0] ^ isometries_nodes1[2][0]\n",
    "            input_nodes[12][0] ^ isometries_nodes1[2][2]\n",
    "            input_nodes[13][0] ^ isometries_nodes1[2][3]\n",
    "            \n",
    "            input_nodes[11][0] ^ isometries_nodes1[3][1]\n",
    "            input_nodes[14][0] ^ isometries_nodes1[3][2]\n",
    "            input_nodes[15][0] ^ isometries_nodes1[3][3]\n",
    "            \n",
    "            \n",
    "            isometries_nodes1[0][4] ^ e_nodes2[0]\n",
    "            isometries_nodes1[1][4] ^ e_nodes2[1]\n",
    "            isometries_nodes1[2][4] ^ e_nodes2[2]\n",
    "            isometries_nodes1[3][4] ^ e_nodes2[3]\n",
    "\n",
    "            e_nodes2[4] ^ isometries_nodes2[0]\n",
    "            e_nodes2[5] ^ isometries_nodes2[1]\n",
    "            e_nodes2[6] ^ isometries_nodes2[2]\n",
    "            e_nodes2[7] ^ isometries_nodes2[3]\n",
    "\n",
    "                            \n",
    "            nodes = tn.reachable(isometries_nodes2)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            #result = (c @ b).tensor\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        output = tf.vectorized_map(lambda vec: f(vec, self.entanglers1, self.entanglers2,\n",
    "                                                 self.isometries1,  self.isometries2, self.bias, self.kernel_dims), inputs)\n",
    "        return tf.reshape(output, (-1, self.output_dims))\n",
    "    \n",
    "from tensorflow.keras.layers import Lambda, Input, Concatenate, Reshape, Softmax\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "def get_model(input_shape=(64, 64, 1)):\n",
    "    x_in = Input(shape=input_shape)\n",
    "    x_out_list = []\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            subx = Lambda(lambda x:x[:, 8*i:8*(i+1),8*j:8*(j+1),:] )(x_in)\n",
    "            x_out_list.append(GridMERAin16(kernel_dims=8, bond_dims=2, output_dims=1)(subx))\n",
    "    x_out = Concatenate(axis=1)(x_out_list)\n",
    "    x_out = Reshape(target_shape=(8, 8, 1))(x_out)\n",
    "    y = GridMERAin16(kernel_dims=8, bond_dims=2, output_dims=2)(x_out)\n",
    "    y = Softmax()(y)\n",
    "    return Model(inputs=x_in, outputs=y)\n",
    "\n",
    "\n",
    "MERA_model_64 = get_model(input_shape=(64, 64, 1))\n",
    "\n",
    "MERA_model_64.summary()\n",
    "\n",
    "# TensorNetwork model\n",
    "MERA_model_64.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.00001), metrics=['accuracy'])\n",
    "MERA_model_64_hist = MERA_model_64.fit_generator(train_data, validation_data=val_data, epochs=50, verbose=1)\n",
    "\n",
    "# TN model\n",
    "MERA_model_64.evaluate_generator(test_data, int(np.ceil(1000/batch_size)), shuffle = false, pickle_safe = True, workers = 1)\n",
    "\n",
    "tn_loss = MERA_model_64_hist.history['loss']\n",
    "tn_acc = MERA_model_64_hist.history['accuracy']\n",
    "\n",
    "\n",
    "np.savetxt('loss_MERA64.out', tn_loss, delimiter=',')  \n",
    "np.savetxt('acc_MERA64.out', tn_acc, delimiter=',')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
