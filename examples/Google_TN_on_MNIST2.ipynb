{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XnVMPBXmtRa"
   },
   "source": [
    "# TensorNetworks in Neural Networks.\n",
    "\n",
    "Training the MNIST dataset using tensornetwork package from google.\n",
    "#### Reference: https://github.com/google/TensorNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "7HGRsYNAFxME",
    "outputId": "9189982e-51fe-4bb3-9124-2f01a3c933c8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST dataset from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(\"/home/xiaochen/Fanjie/scripts/TensorNetwork/data/mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000, 10)\n",
      "(10000, 28, 28, 1) (10000, 10)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "x_train = x_train.reshape((60000, 28, 28, 1))\n",
    "y_train = to_categorical(y_train, 10)\n",
    "x_test = x_test.reshape((10000, 28, 28, 1))\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1OMCo5XmrYu"
   },
   "source": [
    "# TensorNetwork layer definition\n",
    "\n",
    "Here, we define the TensorNetwork layer we wish to use to replace the fully connected layer. Here, we simply use a 2 node Matrix Product Operator network to replace the normal dense weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1000000.0, shape=(), dtype=float64)\n",
      "tf.Tensor(1000000.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "def one_edge_at_a_time(a, b):\n",
    "  node1 = tn.Node(a)\n",
    "  node2 = tn.Node(b)\n",
    "  edge1 = node1[0] ^ node2[0]\n",
    "  edge2 = node1[1] ^ node2[1]\n",
    "  tn.contract(edge1)\n",
    "  result = tn.contract(edge2)\n",
    "  return result.tensor\n",
    "\n",
    "def use_contract_between(a, b):\n",
    "  node1 = tn.Node(a)\n",
    "  node2 = tn.Node(b)\n",
    "  node1[0] ^ node2[0]\n",
    "  node1[1] ^ node2[1]\n",
    "  # This is the same as \n",
    "  # tn.contract_between(node1, node2)\n",
    "  result = node1 @ node2\n",
    "  return result.tensor\n",
    "\n",
    "a = np.ones((1000, 1000))\n",
    "b = np.ones((1000, 1000))\n",
    "\n",
    "print(one_edge_at_a_time(a, b))\n",
    "print(use_contract_between(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=545, shape=(10,), dtype=float64, numpy=\n",
       "array([1000000., 1000000., 1000000., 1000000., 1000000., 1000000.,\n",
       "       1000000., 1000000., 1000000., 1000000.])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = [one_edge_at_a_time(a, b) for i in range(10)]\n",
    "tf.stack(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules for ConvAC\n",
    "Tensor Network version of CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNConvLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, fov, inputShape,name):\n",
    "        super(TNConvLayer, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        self.variable = tf.Variable(tf.random.normal(shape=(3, 3), stddev=1.0/9), \n",
    "                                    name=\"conv_kernel_\"+name, trainable=True)\n",
    "        self.fov = fov\n",
    "        self.inputShape = inputShape\n",
    "        self.outputShape = (self.inputShape[0]-fov+1,  self.inputShape[1]-fov+1)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=self.outputShape), name=\"bias\"+name, trainable=True)\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, conv_var, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            #L, W = input_vec.shape[:2]\n",
    "            input_vec = tf.reshape(input_vec, self.inputShape)\n",
    "            result_list = []\n",
    "            for i in range(0,  self.outputShape[0]):\n",
    "                for j in range(0, self.outputShape[1]):\n",
    "                    d = tn.Node(input_vec[i:i+3, j:j+3])\n",
    "                    v = tn.Node(conv_var)\n",
    "                    d[0] ^ v[0]\n",
    "                    d[1] ^ v[1]\n",
    "                    result = v @ d\n",
    "                    result_list.append(result.tensor)\n",
    "            final_result = tf.stack(result_list)\n",
    "            final_result = tf.reshape(final_result, (self.outputShape[0], self.outputShape[1], 1))\n",
    "            # Now we create the network.\n",
    "            # Finally, add bias.\n",
    "            return result\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.variable, self.bias), inputs)\n",
    "        return tf.nn.relu(self.bias + tf.reshape(result, (-1, self.outputShape[0], self.outputShape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvSMKtPufnLp"
   },
   "outputs": [],
   "source": [
    "class TNLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super(TNLayer, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        self.dim = dim\n",
    "        self.a_var = tf.Variable(tf.random.normal(shape=(dim, dim, 2), stddev=1.0/dim), name=\"a\", trainable=True)\n",
    "        self.b_var = tf.Variable(tf.random.normal(shape=(dim, dim, 2), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(dim, dim)), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, a_var, b_var, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            input_vec = tf.reshape(input_vec, (self.dim, self.dim))\n",
    "\n",
    "            # Now we create the network.\n",
    "            a = tn.Node(a_var)\n",
    "            b = tn.Node(b_var)\n",
    "            x_node = tn.Node(input_vec)\n",
    "            a[1] ^ x_node[0]\n",
    "            b[1] ^ x_node[1]\n",
    "            a[2] ^ b[2]\n",
    "\n",
    "            # The TN should now look like this\n",
    "            #   |     |\n",
    "            #   a --- b\n",
    "            #    \\   /\n",
    "            #      x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            c = a @ x_node\n",
    "            result = (c @ b).tensor\n",
    "\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.a_var, self.b_var, self.bias), inputs)\n",
    "        return tf.nn.relu(tf.reshape(result, (-1, self.dim**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriNodeTNLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim, n_nodes):\n",
    "        super(TriNodeTNLayer, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        self.dim = dim\n",
    "        self.a_var = tf.Variable(tf.random.normal(shape=(11, 11, 2), stddev=1.0/dim), name=\"a\", trainable=True)\n",
    "        self.b_var = tf.Variable(tf.random.normal(shape=(11, 11, 2, 2), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.c_var = tf.Variable(tf.random.normal(shape=(16, 16, 2), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        \n",
    "        self.bias = tf.Variable(tf.zeros(shape=(11, 11, 16)), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, a_var, b_var, c_var, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            input_vec = tf.reshape(input_vec, (11, 11, 16))\n",
    "\n",
    "            # Now we create the network.\n",
    "            a = tn.Node(a_var)\n",
    "            b = tn.Node(b_var)\n",
    "            c = tn.Node(c_var)\n",
    "            x_node = tn.Node(input_vec)\n",
    "            a[1] ^ x_node[0]\n",
    "            b[1] ^ x_node[1]\n",
    "            c[1] ^ x_node[2]\n",
    "            a[2] ^ b[2]\n",
    "            b[3] ^ c[2]\n",
    "            \n",
    "\n",
    "            # The TN should now look like this\n",
    "            #   |     |     |\n",
    "            #   a --- b --- c\n",
    "            #    \\   |     /\n",
    "            #        x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            # c1 = a @ x_node # avoid trace edge?\n",
    "            # c2 = c1 @ b\n",
    "            c1 = a @ x_node\n",
    "            c2 = c1 @ b\n",
    "            c3 = c2 @ c\n",
    "            result = c3.tensor\n",
    "            #nodes = tn.reachable(x_node)\n",
    "            #result = tn.contractors.greedy(nodes)\n",
    "            print(result)\n",
    "            \n",
    "            #result = (c @ b).tensor\n",
    "\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.a_var, self.b_var, self.c_var, self.bias), inputs)\n",
    "        return tf.nn.relu(tf.reshape(result, (-1, self.dim**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNodeTNLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super(MultiNodeTNLayer, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        self.dim = dim\n",
    "        assert len(self.dim) > 2, \"This layer is only used for where the number of input dims is larger than 3.\"\n",
    "        self.var_list = [tf.Variable(tf.random.normal(shape=(dim[0], dim[0], 2), stddev=1.0/(2*dim[0])), \n",
    "                                     name=\"var_\"+str(1), trainable=True)] \n",
    "        self.var_list.extend([tf.Variable(tf.random.normal(shape=(dim[i], dim[i], 2, 2), stddev=1.0/(2*dim[i])), \n",
    "                                     name=\"var_\"+str(i+1), trainable=True) for i in range(1, len(dim)-1)])\n",
    "        self.var_list.extend([tf.Variable(tf.random.normal(shape=(dim[-1], dim[-1], 2), stddev=1.0/(2*dim[-1])), \n",
    "                                     name=\"var_\"+str(len(dim)), trainable=True)])\n",
    "    \n",
    "        self.bias = tf.Variable(tf.zeros(shape=dim), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, var_list, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            input_vec = tf.reshape(input_vec, self.dim)\n",
    "\n",
    "            # Now we create the network.\n",
    "            nodes_list = [tn.Node(var_list[i]) for i in range(len(var_list))]\n",
    "            x_node = tn.Node(input_vec)\n",
    "            #assert False\n",
    "            var_list[0][1] ^ x_node[0]\n",
    "            \n",
    "            for i in range(len(self.dim)):\n",
    "                var_list[i][1] ^ x_node[i]\n",
    "            \n",
    "            assert False\n",
    "            var_list[0][2] ^ var_list[1][2]\n",
    "            for i in range(1, len(var_list)-1):\n",
    "                var_list[i][3] ^ var_list[i+1][2]\n",
    "                \n",
    "#             a[1] ^ x_node[0]\n",
    "#             b[1] ^ x_node[1]\n",
    "#             c[1] ^ x_node[2]\n",
    "#             a[2] ^ b[2]\n",
    "#             b[3] ^ c[2]\n",
    "            \n",
    "\n",
    "            # The TN should now look like this\n",
    "            #     |     |     |\n",
    "            #    a --- ... --- c\n",
    "            #    \\ ...  | ...  /\n",
    "            #           x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            # c1 = a @ x_node # avoid trace edge?\n",
    "            # c2 = c1 @ b\n",
    "            \n",
    "            result = var_list[0] @ x_node\n",
    "            for i in range(1, len(var_list)):\n",
    "                result @= var_list[i]\n",
    "            #c2 = c1 @ b\n",
    "            #c3 = c2 @ c\n",
    "            result = result.tensor\n",
    "            #nodes = tn.reachable(x_node)\n",
    "            #result = tn.contractors.greedy(nodes)\n",
    "            #print(result)\n",
    "            \n",
    "            #result = (c @ b).tensor\n",
    "\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.var_list, self.bias), inputs)\n",
    "        return tf.nn.relu(tf.reshape(result, (-1, self.dim**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-CVqIhPnhY_"
   },
   "source": [
    "# Dense model vs. TN model\n",
    "The TN layer has nearly 100x fewer parameters in this particular architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TNConvLayer.call of <__main__.TNConvLayer object at 0x7f388419e610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TNConvLayer.call of <__main__.TNConvLayer object at 0x7f388419e610>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method TNConvLayer.call of <__main__.TNConvLayer object at 0x7f388419e610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TNConvLayer.call of <__main__.TNConvLayer object at 0x7f388419e610>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "in converted code:\n\n    <ipython-input-45-50bdc692682a>:37 call\n        result = tf.vectorized_map(lambda vec: f(vec, self.variable, self.bias), inputs)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:338 vectorized_map\n        return pfor(loop_fn, batch_size)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:164 pfor\n        return f()\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:161 f\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:214 _pfor_impl\n        outputs.append(converter.convert(loop_fn_output))\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1175 convert\n        output = self._convert_helper(y)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1208 _convert_helper\n        assert isinstance(y, ops.Tensor), y\n\n    AssertionError: __unnamed_node__\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-42c4a7c25fab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTNConvLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputShape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    628\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: in converted code:\n\n    <ipython-input-45-50bdc692682a>:37 call\n        result = tf.vectorized_map(lambda vec: f(vec, self.variable, self.bias), inputs)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:338 vectorized_map\n        return pfor(loop_fn, batch_size)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:164 pfor\n        return f()\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:161 f\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:214 _pfor_impl\n        outputs.append(converter.convert(loop_fn_output))\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1175 convert\n        output = self._convert_helper(y)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1208 _convert_helper\n        assert isinstance(y, ops.Tensor), y\n\n    AssertionError: __unnamed_node__\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Input\n",
    "\n",
    "tn_model = tf.keras.Sequential()\n",
    "tn_model.add(Input(shape=(28, 28, 1)))\n",
    "tn_model.add(TNConvLayer(fov=3, inputShape=(28, 28),name='conv1'))\n",
    "tn_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', name='conv2'))\n",
    "tn_model.add(Conv2D(16, kernel_size=(3,3), activation='relu', name='conv3'))\n",
    "tn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "tn_model.add(tf.keras.layers.Flatten())\n",
    "tn_model.add(TNLayer(44))\n",
    "tn_model.add(Dense(10, activation='softmax', name='fc2'))\n",
    "tn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "bbKsmK8wIFTp",
    "outputId": "a4f49baa-1087-4d2d-b00e-2c8349c51086"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b8bee65d7eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf'"
     ]
    }
   ],
   "source": [
    "from tf.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "tn_model = tf.keras.Sequential()\n",
    "tn_model.add(Conv2D(16, kernel_size=(3,3), activation='relu', name='conv1', input_shape=(28,28,1)))\n",
    "tn_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', name='conv2'))\n",
    "tn_model.add(Conv2D(16, kernel_size=(3,3), activation='relu', name='conv3'))\n",
    "tn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "tn_model.add(tf.keras.layers.Flatten())\n",
    "tn_model.add(TNLayer(44))\n",
    "tn_model.add(Dense(10, activation='softmax', name='fc2'))\n",
    "tn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_model2 = tf.keras.Sequential()\n",
    "tn_model2.add(Conv2D(16, kernel_size=(3,3), activation='relu', name='conv1', input_shape=(28,28,1)))\n",
    "tn_model2.add(Conv2D(32, kernel_size=(3,3), activation='relu', name='conv2'))\n",
    "tn_model2.add(Conv2D(16, kernel_size=(3,3), activation='relu', name='conv3'))\n",
    "tn_model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "tn_model2.add(tf.keras.layers.Flatten())\n",
    "tn_model2.add(TriNodeTNLayer(44, 3))\n",
    "tn_model2.add(Dense(10, activation='softmax', name='fc2'))\n",
    "tn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiNodeTNLayer.call of <__main__.MultiNodeTNLayer object at 0x7efb50623610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiNodeTNLayer.call of <__main__.MultiNodeTNLayer object at 0x7efb50623610>>: ValueError: Failed to parse source code of <bound method MultiNodeTNLayer.call of <__main__.MultiNodeTNLayer object at 0x7efb50623610>>, which Python reported as:\n",
      "    def call(self, inputs):\n",
      "        # Define the contraction.\n",
      "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
      "        def f(input_vec, var_list, bias_var):\n",
      "            # Reshape to a matrix instead of a vector.\n",
      "            input_vec = tf.reshape(input_vec, self.dim)\n",
      "\n",
      "            # Now we create the network.\n",
      "            nodes_list = [tn.Node(var_list[i]) for i in range(len(var_list))]\n",
      "            x_node = tn.Node(input_vec)\n",
      "            #assert False\n",
      "            var_list[0][1] ^ x_node[0]\n",
      "\n",
      "            for i in range(len(self.dim)):\n",
      "                var_list[i][1] ^ x_node[i]\n",
      "\n",
      "            assert False\n",
      "            var_list[0][2] ^ var_list[1][2]\n",
      "            for i in range(1, len(var_list)-1):\n",
      "                var_list[i][3] ^ var_list[i+1][2]\n",
      "\n",
      "#             a[1] ^ x_node[0]\n",
      "#             b[1] ^ x_node[1]\n",
      "#             c[1] ^ x_node[2]\n",
      "#             a[2] ^ b[2]\n",
      "#             b[3] ^ c[2]\n",
      "\n",
      "\n",
      "            # The TN should now look like this\n",
      "            #     |     |     |\n",
      "            #    a --- ... --- c\n",
      "            #    \\ ...  | ...  /\n",
      "            #           x\n",
      "\n",
      "            # Now we begin the contraction.\n",
      "            # c1 = a @ x_node # avoid trace edge?\n",
      "            # c2 = c1 @ b\n",
      "\n",
      "            result = var_list[0] @ x_node\n",
      "            for i in range(1, len(var_list)):\n",
      "                result @= var_list[i]\n",
      "            #c2 = c1 @ b\n",
      "            #c3 = c2 @ c\n",
      "            result = result.tensor\n",
      "            #nodes = tn.reachable(x_node)\n",
      "            #result = tn.contractors.greedy(nodes)\n",
      "            #print(result)\n",
      "\n",
      "            #result = (c @ b).tensor\n",
      "\n",
      "            # Finally, add bias.\n",
      "            return result + bias_var\n",
      "\n",
      "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
      "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
      "        result = tf.vectorized_map(lambda vec: f(vec, self.var_list, self.bias), inputs)\n",
      "        return tf.nn.relu(tf.reshape(result, (-1, self.dim**2)))\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiNodeTNLayer.call of <__main__.MultiNodeTNLayer object at 0x7efb50623610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiNodeTNLayer.call of <__main__.MultiNodeTNLayer object at 0x7efb50623610>>: ValueError: Failed to parse source code of <bound method MultiNodeTNLayer.call of <__main__.MultiNodeTNLayer object at 0x7efb50623610>>, which Python reported as:\n",
      "    def call(self, inputs):\n",
      "        # Define the contraction.\n",
      "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
      "        def f(input_vec, var_list, bias_var):\n",
      "            # Reshape to a matrix instead of a vector.\n",
      "            input_vec = tf.reshape(input_vec, self.dim)\n",
      "\n",
      "            # Now we create the network.\n",
      "            nodes_list = [tn.Node(var_list[i]) for i in range(len(var_list))]\n",
      "            x_node = tn.Node(input_vec)\n",
      "            #assert False\n",
      "            var_list[0][1] ^ x_node[0]\n",
      "\n",
      "            for i in range(len(self.dim)):\n",
      "                var_list[i][1] ^ x_node[i]\n",
      "\n",
      "            assert False\n",
      "            var_list[0][2] ^ var_list[1][2]\n",
      "            for i in range(1, len(var_list)-1):\n",
      "                var_list[i][3] ^ var_list[i+1][2]\n",
      "\n",
      "#             a[1] ^ x_node[0]\n",
      "#             b[1] ^ x_node[1]\n",
      "#             c[1] ^ x_node[2]\n",
      "#             a[2] ^ b[2]\n",
      "#             b[3] ^ c[2]\n",
      "\n",
      "\n",
      "            # The TN should now look like this\n",
      "            #     |     |     |\n",
      "            #    a --- ... --- c\n",
      "            #    \\ ...  | ...  /\n",
      "            #           x\n",
      "\n",
      "            # Now we begin the contraction.\n",
      "            # c1 = a @ x_node # avoid trace edge?\n",
      "            # c2 = c1 @ b\n",
      "\n",
      "            result = var_list[0] @ x_node\n",
      "            for i in range(1, len(var_list)):\n",
      "                result @= var_list[i]\n",
      "            #c2 = c1 @ b\n",
      "            #c3 = c2 @ c\n",
      "            result = result.tensor\n",
      "            #nodes = tn.reachable(x_node)\n",
      "            #result = tn.contractors.greedy(nodes)\n",
      "            #print(result)\n",
      "\n",
      "            #result = (c @ b).tensor\n",
      "\n",
      "            # Finally, add bias.\n",
      "            return result + bias_var\n",
      "\n",
      "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
      "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
      "        result = tf.vectorized_map(lambda vec: f(vec, self.var_list, self.bias), inputs)\n",
      "        return tf.nn.relu(tf.reshape(result, (-1, self.dim**2)))\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    <ipython-input-67-7ed9bfbf5470>:76 call\n        result = tf.vectorized_map(lambda vec: f(vec, self.var_list, self.bias), inputs)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:338 vectorized_map\n        return pfor(loop_fn, batch_size)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:164 pfor\n        return f()\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:161 f\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:198 _pfor_impl\n        loop_fn_outputs = loop_fn(loop_var)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:336 loop_fn\n        return fn(gathered_elems)\n    <ipython-input-67-7ed9bfbf5470>:76 <lambda>\n        result = tf.vectorized_map(lambda vec: f(vec, self.var_list, self.bias), inputs)\n    <ipython-input-67-7ed9bfbf5470>:32 f\n        var_list[0][1] ^ x_node[0]\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:888 binary_op_wrapper\n        y, dtype_hint=x.dtype.base_dtype, name=\"y\")\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1145 convert_to_tensor_v2\n        as_ref=False)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1224 internal_convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:305 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:246 constant\n        allow_broadcast=True)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:284 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:562 make_tensor_proto\n        \"supported type.\" % (type(values), values))\n\n    TypeError: Failed to convert object of type <class 'tensornetwork.network_components.Edge'> to Tensor. Contents: __unnamed_edge__. Consider casting elements to a supported type.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-d9136e00d5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtn_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtn_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtn_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiNodeTNLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtn_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fc2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtn_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    628\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-67-7ed9bfbf5470>:76 call\n        result = tf.vectorized_map(lambda vec: f(vec, self.var_list, self.bias), inputs)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:338 vectorized_map\n        return pfor(loop_fn, batch_size)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:164 pfor\n        return f()\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:161 f\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:198 _pfor_impl\n        loop_fn_outputs = loop_fn(loop_var)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:336 loop_fn\n        return fn(gathered_elems)\n    <ipython-input-67-7ed9bfbf5470>:76 <lambda>\n        result = tf.vectorized_map(lambda vec: f(vec, self.var_list, self.bias), inputs)\n    <ipython-input-67-7ed9bfbf5470>:32 f\n        var_list[0][1] ^ x_node[0]\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:888 binary_op_wrapper\n        y, dtype_hint=x.dtype.base_dtype, name=\"y\")\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1145 convert_to_tensor_v2\n        as_ref=False)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1224 internal_convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:305 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:246 constant\n        allow_broadcast=True)\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:284 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /home/xiaochen/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:562 make_tensor_proto\n        \"supported type.\" % (type(values), values))\n\n    TypeError: Failed to convert object of type <class 'tensornetwork.network_components.Edge'> to Tensor. Contents: __unnamed_edge__. Consider casting elements to a supported type.\n"
     ]
    }
   ],
   "source": [
    "tn_model3 = tf.keras.Sequential()\n",
    "tn_model3.add(Conv2D(16, kernel_size=(3,3), activation='relu', name='conv1', input_shape=(28,28,1)))\n",
    "tn_model3.add(Conv2D(32, kernel_size=(3,3), activation='relu', name='conv2'))\n",
    "tn_model3.add(Conv2D(16, kernel_size=(3,3), activation='relu', name='conv3'))\n",
    "tn_model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "tn_model3.add(tf.keras.layers.Flatten())\n",
    "tn_model3.add(MultiNodeTNLayer((11, 11, 4, 4)))\n",
    "tn_model3.add(Dense(10, activation='softmax', name='fc2'))\n",
    "tn_model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWwoYp0WnsLA"
   },
   "source": [
    "# Training the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.0818 - accuracy: 0.9756\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0444 - accuracy: 0.9865\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 77s 1ms/sample - loss: 0.0352 - accuracy: 0.9890\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 80s 1ms/sample - loss: 0.0306 - accuracy: 0.9906\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.0258 - accuracy: 0.9919\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.0258 - accuracy: 0.9921\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0225 - accuracy: 0.9930\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0222 - accuracy: 0.9934\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0193 - accuracy: 0.9941\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0219 - accuracy: 0.9935\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 76s 1ms/sample - loss: 0.0184 - accuracy: 0.9945\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0219 - accuracy: 0.9937\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 76s 1ms/sample - loss: 0.0203 - accuracy: 0.9941\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 76s 1ms/sample - loss: 0.0206 - accuracy: 0.9944\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0177 - accuracy: 0.9953\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0152 - accuracy: 0.9957\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0197 - accuracy: 0.9946\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0172 - accuracy: 0.9950\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0182 - accuracy: 0.9954\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0168 - accuracy: 0.9955\n",
      "CPU times: user 1h 28min 18s, sys: 16min 36s, total: 1h 44min 55s\n",
      "Wall time: 25min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TensorNetwork model\n",
    "tn_model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "tn_hist = tn_model.fit(x_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 84s 1ms/sample - loss: 0.1515 - accuracy: 0.9531\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 86s 1ms/sample - loss: 0.0531 - accuracy: 0.9837\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0392 - accuracy: 0.9877\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0342 - accuracy: 0.9888\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0290 - accuracy: 0.9908\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 84s 1ms/sample - loss: 0.0261 - accuracy: 0.9918\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0220 - accuracy: 0.9928\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.0206 - accuracy: 0.9935\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0184 - accuracy: 0.9938\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0169 - accuracy: 0.9945s - loss: 0\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0150 - accuracy: 0.9948\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 86s 1ms/sample - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 75s 1ms/sample - loss: 0.0145 - accuracy: 0.9950\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.0128 - accuracy: 0.9956\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.0128 - accuracy: 0.9959\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 87s 1ms/sample - loss: 0.0117 - accuracy: 0.9962\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0123 - accuracy: 0.9960\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0120 - accuracy: 0.9961\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.0108 - accuracy: 0.9966\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 86s 1ms/sample - loss: 0.0108 - accuracy: 0.9964\n",
      "CPU times: user 1h 26min 40s, sys: 17min, total: 1h 43min 40s\n",
      "Wall time: 28min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TensorNetwork model\n",
    "tn_model2.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "tn_hist2 = tn_model2.fit(x_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMxSJo5gtOmQ"
   },
   "source": [
    "# Tests and performance graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a7a5a04c16de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \"\"\"\n\u001b[1;32m    868\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0;31m# Case 1: distribution strategy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchkfj/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2932\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2934\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   2935\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "# TN model\n",
    "tn_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 729us/sample - loss: 0.0416 - accuracy: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04163387740198805, 0.9901]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TN2 model\n",
    "tn_model2.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXwb9Zn48c9j+T7j2I6dxLnvy4SQgxCOpEBIgBIgUAjQA7altNvd0m5LYbullNKlLGwX2rLtdltKsqWQlkJL+REIRwIhhCOEHM5B4sRJLMd2fEW+L+n7+2PGtuxItnzKlp7366WXRjPfGT0ay4++euarGTHGoJRSKnRFBDsApZRSA0sTvVJKhThN9EopFeI00SulVIjTRK+UUiFOE71SSoU4TfRKDQIRqRGRyQOw3f0isry/t9uD598kIl/s77aqf4mOox9+RGQrcA6QZYxpDHI4apgSEQNMM8bkBTsWNbC0Rz/MiMhE4CLAANcM8nNHDubzDSehuG9C8TWFK030w88XgPeBp4EOX4NFJE5E/lNEToiIS0TeFZE4e9mFIvKeiJwRkQIR+ZI9f6uIfNlrG18SkXe9HhsR+UcROQIcsec9YW+jSkQ+FpGLvNo7RORfReSoiFTby8eJyJMi8p+d4v27iNzt60WKyBwReV1EKkSkRET+1Z4fIyKPi8gp+/a4iMTYy5aLiFNE7hGR0yJSJCLXisiVInLY3ta/ej3HAyLyvIhstGPdJSLneC2/1+t1HBCR6zrtp+0i8l8iUgE8ICJTReRte9+XicjGTvtxqj2dIiIbRKTU/lv9m4hEeO9/EXlMRCpFJF9EVvt7M4jIcRG5zOv1/MnedrVd1lnoZ7137Mk9dlnpJq/99z0RKQZ+LyKpIvKyHWulPZ3ttZ229093sfew7SQRecd+HW/Y758/+NsPqhvGGL0NoxuQB3wdOA9oBjK9lj0JbAXGAg7gAiAGGA9UA+uAKCANmG+vsxX4stc2vgS86/XYAK8DI4E4e95t9jYigX8BioFYe9l3gX3ADECwSkxpwGLgFBBht0sH6rzj93rOJKDI3nas/XiJvexBrA+6UUAG8B7wY3vZcqAFuN9+nV8BSoE/2tuYAzQAk+32D9j78Aa7/XeAfCDKXn4jMAarQ3QTUAuM9tpPLcA/2fshDngW+L7dPha4sNN+nGpPbwD+Zsc0ETgM/IPXdpvt2B3A1+z9Jn7eD8eBy7xeTwNwpb3uw8D7XbyX2mLqtP8ewXrfxNl/u7VAvB3vn4G/eq2zFfv9013sPWy7A3gMiAYuBKqAPwT7/2+43oIegN568Mey3vDNQLr9+BDwLXs6AqgHzvGx3n3Ai3622fbPZz/+Emcn+s90E1dl6/MCnwJr/LQ7CFxuT38DeMVPu3XAJ36WHQWu9Hp8BXDcnl5u7wOH/TjJjn+JV/uPgWvt6Qe8E6G9D4uAi/w89+7W12bvp5Odlm8AfgNk+1jXAFPtpNYIzPZa9lVgq9d287yWxdvrZvmJ6TgdE/0bXstmA/Vd/N18Jfom7A9tP+vMByp9vX+6iz3QtlgdkxYg3mv5H9BE3+ublm6Gly8Cm40xZfbjP9JevknH6kUe9bHeOD/zA1Xg/UBE/kVEDtolijNAiv383T3XeqxvA9j3/+enXVfbGAOc8Hp8wp7XqtwY47an6+37Eq/l9UCi1+O212aM8QDO1u2JyBdEZLdY5a4zwFzaX2eHdW33YH2L+dAum9zhI/50rF5q59cw1utxsVdMdfakd8xdKfaargNipWe19lJjTEPrAxGJF5H/sUtMVcA7wAgRcXT3/AHE7q/tGKDCax6cva9VD2iiHybEqrV/DrhERIrtGuq3gHPsunIZ1tf2KT5WL/AzH6xyRLzX4ywfbdqGZolVj/+eHUuqMWYE4MJKcN091x+ANXa8s4C/+mnX1TZOARO8Ho+35/XWuNYJu06eDZwSkQnA/2J980izX2cu7a8TvPYLgDGm2BjzFWPMGKxe+n+31uW9lGF9K+v8Ggr78Br6U+dheP+CVYZbYoxJBi625wsDpwgYKSLe78tx/hqr7mmiHz6uBdxYX8fn27dZwDbgC3Zv9CngZyIyRqyDokvtA5XPAJeJyOdEJFJE0kRkvr3d3cD1ds9tKvAP3cSRhPW1uhSIFJH7gWSv5b8Ffiwi08SSIyJpAMYYJ/ARVk/+L8aYenx7GcgSkbvFOviaJCJL7GXPAv8mIhkiko5Vj+/LQbrzROR6u9d7N1ZZ5X0gASvplQKIyO1YPXq/RORGrwOVlfb6bu829reNPwE/sV/XBODbfXwNvVUCdDe2PwnrW9AZERkJ/HCggzLGnAB2Yh3gjhaRpcBnB/p5Q5km+uHji8DvjTEn7Z5jsTGmGPglcKudqL6DdSD0I6AC66BahDHmJNYBun+x5+/GOkgK8F9YddkSrNLKM93E8RqwCesA4gmsbxHeX6t/hpXINmMdQPsd1kG9VuuBefgv22CMqQYux/rnLsYa7bPCXvwQVhLYa7/WXfa83vob1oHWSuDzwPXGmGZjzAHgP7EOCpbYMW/vZluLgA9EpAZ4CfimMSbfR7t/wvomdQx4F6sE91QfXkNvPQCst0tTn/PT5nGsv18Z1gfgq4MU263AUqAc6++7EetDWPWC/mBKDSoRuRir9zrR/hYSzFgewDoYeVt3bVVw2UNVDxljBvwbRSjSHr0aNCISBXwT+G2wk7wa2kRkkYhMEZEIEVkFrMH/MR3VDf3lmxoUIjILq+SyB7g9yOGooS8LeAFrHL8T+Jox5pPghjR8aelGKaVCnJZulFIqxA250k16erqZOHFisMNQSqlh5eOPPy4zxmT4WjbkEv3EiRPZuXNnsMNQSqlhRURO+FumpRullApxmuiVUirEaaJXSqkQp4leKaVCnCZ6pZQKcZrolVIqxGmiV0qpEDfkxtErpdSw4fGAqwDKjkDZYWiug/g0iB8JcSM7TkdGBy1MTfRKKdWdplooz2tP6GWHrenyPGhp6H59gOgkiE89+wOgbToVUsbB+CXdb6uHNNErpYKjpQkazkD9mfZ7DETFW7fo1vsEe14cyABewdDjgZqSjom89b7K2d5OImDEBEifDpOXW/ett+gEqK+AugqoK7eny6Gu0mu6wpquOGrNb3S1bzt7EXz5jX5/aZrolVJ909JoJcjqEqgrgwZXx+Tt777F35Uk/RGvD4A4iEo4+8NAIsDdBO5m+9572nte09nLPS0dny46EdKnwcRl1n36dEibBiMnQ1Ss/zCjxkDyGP/LO3M3Q32l9QEwQJdp0ESvlPKtuR6qi+0kXmxPF1sJvbqofX59hf9tRCdB3AiIHWHdp03p+DjWujVFJVHaEkdSXAzJjibruZtqrZp3Ux0019r3dfb8+o7zqk5Z98aAIxocUfZ9NETGQEzS2fPbpr3mxadBht07Txo9sN8gWjmiIHGUdRsgmuiVCjZXIRR+DKd2QUSk1WtMmwrpUyE2ZeCe190MlSc6lipcBXYCL7J65p1FREJiFiRlQuokGL8UkrIgMdO6T0hvS97EpoDDSjG1jS0UnqnHWVlHYWU9zjP1OJ311nRlPWU1jUANEVLDwokjWTk7m5WzsxifFj9wrz8IPB5Dk9tDY4uHphYPTW7rvrHFTVOLh5hIBzOykvr9eYfchUcWLlxo9OyVKmQ1VMGpT6BwJxTushJ8dZG1LCLS6pEad3v7hFHtSb/tA2AapE60eoKBqD9jH0jsVHuuyAdPc3u7xEwYMd5O3FnWfefpuJEQcfaobGMMx8vrOFJSjbOyvj2pn7GSeWVdc4f20Y4IxoyIJTs1nrEj4shOjWPMiDhOlNey+UAJh4qrAZiZlcTK2ZmsnJPFnDHJyGD0sLvQ2OLmdFUjp6sbKa1uoKSqkdNt941U1Tf7TOCt85rdXefb+eNG8Nd/XNar2ETkY2PMQp/LNNErNUDczVCyvz2pO3daCRb7f27kFBh7HmQvtO4z51o15sr89hEe5Ueg/Kg1XVfWvm1xWMk+3U7+rR8AzQ3tCb01udeUtK8XEWk9b2vNua32PNUqpfRAkaue9/LK2X60jB1HyylytY8+iY2K6JDEx6bGdXickRhDRIT/pH2yvI7NB4rZfKCEnccr8BgYkxLL5XbSXzxpJFGO/vsZUIvbQ3FVA8WuBk5XN1JSZd2fthP56apGSqobONPpAwvAESFkJMYwKjmGlLgoYiIjiI6MINph3cdEOqzHkREdlsX4WJaaEM2C8am9eg2a6JUaSMZYBxiri+3E/rGV1Iv3tg+9i0/3SuoLYMwCa0hdT9RXtif9cntoX1meNXqj8xC/2BGQMcP6FtCW1KdD6oTAvwl0UlHbxPvHytmeZyX2Y2W1AKTGR3HBlHSWTklj3tgUslPjGJkQ3W+974raJt48WMLmAyVsO1JKQ7OH5NhIPjNzFCvnZHHx9AwSY7quQje2uCk603B2+ajS+sZRXNWA29MxF0Y5WhN4LKOSrESemRTLqOQYRnndj0yIxtHFh9Zg0USvVG80N1i94ZrT9n3rdLHXPPve3dS+XmQsjJ7fntTHLrRKIgNVdvB4rOF/5XngiLESfHxan5+vprGFD/PLeS+vnPeOlnOgqAqAhGgHSyanccGUNC6Yks7MrKQue+f9qb7JzbYjpWw+UMKbB0uorGsmOjKCZVPSWDkni9EpsW2lI6v+b5WPTlc34p3qIgSyku3SUar9rWNEHFkpsWQmW7cRcVGD9rr6gyZ6pfypLYPifVCSa/XGXc72pO7rYCRiHXBMzITEUTTFjeJkUyK5rlg+LoukOHoCGVPms2jKKJZMSmPMiLhBf0m9YYyhtsnNXucZdhy1eu17nC7cHkN0ZATnjU9l2dQ0lk5JJyc7pV/LJr3V4vaw80Qlrx8oYfOBYgoq2odrRjmE0SntCbxz6SgrJXZIvIb+pIleKY/b6vG2JvXiXOu+9UAoWMPpUifZQ90y2++TstqmTXwaB0vq2fLpabZ+eppdJ8/g9hhS4qK4aFo6Dc1uPsyvoKrBGpOdnRrHkklpLJk8kvMnpTFuZNygHFB0ewxn6pqoqPW61TVRUdNEeW0TlZ2X1TbR2GKN4XZECDnZKW099vMmpBIb5RjwmPvCGMOnJdXUNLQwNjWOUUmxQ6KcMpg00auQ5PYYjpXWkHvKxf7CKg4WVxEb6WBWquGc6EKmmuNk1R0mtuIgcvpgex07Isoqb2TNsw6AZs2FzHmQkObzeaobmtmeV8aWQ6VsPXyakqpGAOaOTWbFjFEsn5HB/HGpbYnF7TEcKq7ig2MVfJhfwYfHK6iotUo7o1NiWTxpZFvyn5ye0KPE7/YYymoaKXI1UOyqp9jVQJF9ELHI1UBZTSOVtU2cqW/G3792UkwkIxOjSY2PJi0hmtSE9vupGYksnjyS5Nje1fFV8GiiV8NeY4ubIyU15DrPcKzgBKWFJ6gtK2Ckp5xMKhnjqGRyTBXjWk6SZU63rVdukjjCRIrjplKbOhMy55EyYQ6TMlOZlJ5AfPTZB/GMMRwuqWHrp6fZ8ulpdh6vpMVjSIqN5OJpGSyfkcElMzIYldTFryO9eDyGvNIaPjhWzvv5FXxwrMIeNw4ZSTF24reSf3y0g+KqBq9E3khxVb392BoJ0vmgYbQjgqyUWLKSY8lIimFkp+SdlmAndTu5R0eGVslCWTTRq+GhuR5chdRXFFBUkE9F8Qnqy51QXUR8YymZUkEGZ4iRlrNWNQkZSFIWpE3DkzmX8sTpHI2YxKGaeI6V15FfVsux0loKz3T82f3olFgmZyQwKT2BiWkJHCurZeuh05yyhwrOzEpi+YxRrJiRwYIJqf1S1zXGkF9Wywf5FXxwrJwP8is6DE30Fh/tICslltEpsWQlx1n3dlJvnd+fI1zU8KWJXg0ZxhhcdU2UFx2n3rkHivYRW3GAEVWHGdlYQAQd34+1xOKKTKc5PhNHyliSMsaRlDGOiJQxVk09abRVRw/wFLD1TW6Ol1tJ/1hpDflltRwts6arG1pIiHZw4bR0ltslmdEpA38w1RhDQUU9Hx6vwO3xkJXSntCTYiI1iauAdJXo9RQIqt9UNTRTWFnf9oOT0upGys9U4Sg/TLLrEKPq8hjffIyZcoIpUtO23klPBntkIs6YC2hJmciIzHGMzp7M5CnTyExPJ6EfE11ctINZo5OZNTq5w3xjDBW1TSTFRg16aUNEGJ8WH3I/91dDhyZ61WvNbg+fnDzDtiOlvHOkjJPOAubIcWbJCWZFnORSOcGUiFNEYf2kv0liKEuawumUyynMmEPE6BwSx+eQkT6K8dHBHdUhIqQlxgQ1BqUGiiZ6FTBjDCfK69oS+46jZWQ35XOZYxePxO5jRswhxC69tCSMRrLm4hh9Y9uolui0KYyJGNrD9JQKRZroVZdc9c3sOFrGO0fK2HaklNMVLpZG7GdN3D4ejd7FCLFHuIxaANPvtc5mmDmXSD9DFZVSg08Tveqgxe1hj9PFO4dL2XaklN0FZ0g3layO2cMvEnKZm7CLSHcDJiIBmbICpl8B01ZaPypSSg1JmuhDiKu+mYraJuqb3NQ3u2lstu7rm900NHus+yY3DZ3mNzS7qW9yU9vUwu6CM9Q0NJETkc+6EQf51chdZNYesp4gajzM/QJMvwKZcGHXV9lRSg0ZmuiHuYZmN68fKOGFXU7eOVJ21o9p/HFECPFRDmKiHMRFR5AW2cASk8s/p+5mXt2HxDaWQn0EjFsCSx+A6asgY+bgXHFHKdWvNNEPQ8YYdp6o5IVdTl7eW0R1QwujU2K58+LJTM9MJC7KQax9i4tyEBftIDbSQWx0RNuyKDzWBTCOvmXdnB9ZF7yISYGpl1qJfeplfk8LoJQaPjTRDyMny+v4yy4nL35SyMmKOuKjHayam8XaBdmcPzmt+5M4VZ6AT+3Env+2fXZGsU6le9G3YfIKGLe41+crV0oNTZrohzhXfTOv7CvihV1OPjpeiQhcMCWNuy+bxhVzskjo6oILjdWQv629115x1JqfnA2zroEpn4HJy3t+AQyl1LCiiX4IanF72HakjOd3OXn9QAlNLR6mZCRwz6oZXDt/rP9znHvcULTbSup5b4HzQ/C0QFQ8TLwIFt9pJff0aVprVyqMaKIfQg4WVfH8x07+tvsUZTWNpMZHsW7RONael828sSn+z3niccMn/wdbHraufgTWFY4u+GcrsY9bDJH6q0+lwlVAiV5EVgFPAA7gt8aYn3ZaPgF4CsgAKoDbjDFOe9kjwFV20x8bYzb2U+whwRjDu3ll/M/bx3g3r4woh3DpzEyuXzCW5TNGdX/elSNvwOs/gNMHrBEyKx+CKSusqyAppRQBJHoRcQBPApcDTuAjEXnJGHPAq9ljwAZjzHoR+QzwMPB5EbkKWADMB2KAt0VkkzGmqr9fyHDT4vbwSm4x//P2UfafqmJUUgz3rp7JTQvHkZoQwJkYS/bD5n+zyjSpE+HG9TB7jZZklFJnCaRHvxjIM8YcAxCR54A1gHeinw18y57eAvzVa/7bxpgWoEVE9gCrgD/1Q+zDUn2Tmz9/XMD/bjtGQUU9kzMSeGTtPK49dywxkQGcB6aqCLb8BHY/AzHJcMW/w6Iva2lGKeVXIIl+LFDg9dgJLOnUZg+wFqu8cx2QJCJp9vwfisjPgHhgBR0/IAAQkTuBOwHGjx/fw5cwPFTWNrF+x3HWv3ecyrpmzh0/gn+7ajaXz8oM7ErzTbXw3i9g+xPgboYlX4OLv6MjZpRS3Qok0fvKQp1/fvkd4Jci8iXgHaAQaDHGbBaRRcB7QCmwAzjr8kDGmN8AvwHrwiMBRz8MFFTU8bt389n4UQH1zW4unTmKu5ZPYeGE1MAuKOFxw+4/wlsPWQdaZ6+Byx6AkZMHOnSlVIgIJNE7gXFej7OBU94NjDGngOsBRCQRWGuMcdnLfgL8xF72R+BI38Me+vafcvGbd47x8t4iBFgzfyxfvWQy0zOTAt/I0bdg8w+gJBeyF8HnNsD4zl+mlFKqa4Ek+o+AaSIyCaunfjNwi3cDEUkHKowxHuA+rBE4rQdyRxhjykUkB8gBNvdj/EOKMYYdR8v51dtH2XakjIRoB3csm8gdF07q2SXpTh+0Enze6zBiPNzwFMy5Xg+0KqV6pdtEb4xpEZFvAK9hDa98yhizX0QeBHYaY14ClgMPi4jBKt38o716FLDNLlFUYQ27PPvKziHgcEk19zy/1zqtb2IM371iBrctmUBKfA9OJ1BdAlv/HXZtgOgka6jk4jv1QKtSqk/04uB95PEY1u84zsObDpEUE8m3V05n7YJsYqMCvJJSTSl8+gocehmObQXjgUVfgUvu0QOtSqmA6cXBB0hJVQPffX4v7xwu5TMzR/HI2hwykgLofVeesBL7wZeh4H0ruY8YbyX4Rf8AaVMGPnilVNjQRN9Lr+YWcd8L+6hvdvPQtXO5dcl4/6NojLF+4HTo/8Ghv0PxPmt+5ly4+B6YdbU1rTV4pdQA0ETfQzWNLTz49/38aaeTeWNTePzm+UzJSDy7ocdjnVTs4N+tBF+ZD0j7aQpmXqVDJJVSg0ITfQ98fKKSb23cjbOyjm+smMo3L5tGlMPrXDQtTZD/jtVrP/QK1J6GiCiYfAks+ybMuBKSMoP3ApRSYUkTfQCa3R5+8VYev3zrCGNGxLHxq0tZNLHTgdKyPFh/NVQXQVQCTLscZn3Wuo9NCU7gSimFJvpuHSut4Vsbd7PH6WLtgmweuGY2SbGdhkzWnIY/XA/uJrj5WevUwHrhbKXUEKGJ3g9jDM9+WMCPXz5AdGQET96ygKtyRp/dsKkW/vg5K9l/6WXI9jm6SSmlgkYTvQ9lNY3c+5e9vHHwNBdOTeexG88hK8VHD93dAn++HYr2wE3PaJJXSg1Jmug7eetQCfc8v5eqhhbuv3o2X7pgou+zSxoD/+/bcOQ1uOpnMPPKwQ9WKaUCoIneZozhwZcP8Pvtx5mZlcQzXz6fGVldnIBs22Owaz1c+G3rR05KKTVEaaK37St08fvtx1m3eBwPXDOn64uAtJ42OOcmuPT+wQtSKaV6oZsLkoaPTbnFOCKEe66Y2XWSP/oWvPRPMOkSuOaX+mtWpdSQp4keq2zzam4xSyendX291qK9sPELkD4Dbvo/iAzg2q5KKRVkmuiBT0uqyS+rZdXcLP+NzhTAMzdCbDLc+mf9EZRSatjQGj2waV8xIrByjp/TE9RXwjM3QHM93PEqpIwd3ACVUqoPNNEDr+0vZuGEVEYl+Rgr39IIz90KFcfgthcgc/bgB6iUUn0Q9qWb/LJaDhVXs2quj1+9ejzw4l1wYjtc+yuYdNHgB6iUUn0U9ol+U24RgO/6/Bv3w/4X4PIHYd4NgxyZUkr1j7BP9K/mFnNOdgpjR3S6ePf7v4b3fmFds/WCfw5OcEop1Q/COtE7K+vY63SdXbY58Dd49V6YeTWs+qmOlVdKDWthnehfzS0GYLV32ebk+/CXr0D2Ilj7W4gI8CLfSik1RIV9op+ZlcTE9ARrRulhePZmSMmGdc9BVFzXG1BKqWEgbBP96aoGPj5ZyerWso27Gf54I0REwm1/gYS04AaolFL9JGzH0b+2vxhjYPU8u2xz5iRUHofPPgEjJwU1NqWU6k9h26PflFvM5IwEpo1KtGZUFVr3qZrklVKhJSwTfUVtEx/kV7B6bhbSOqLG5bTuU7KDF5hSSg2AsEz0rx8oxu0x7fV5AJfdo08eE5yglFJqgIRlot+UW0x2ahxzxiS3z3QVQHy6jrRRSoWcsEv0rvpmtueVdSzbgFWj17KNUioEhV2if+tQCc1uc/avYV1OTfRKqZAUdol+075iMpNjOHfciPaZxmiiV0qFrLBK9LWNLbx9uJRVc7KIiPAq2zS4oKkGkvWCIkqp0BNWiX7rp6U0tnjOLtu0jqHXHr1SKgSFVaLflFvEyIRoFk1M7bhAx9ArpUJY2CT6hmY3Ww6dZuXsTCIdnV62JnqlVAgLm0S/7UgZtU1u31eScjmtk5kl+rk4uFJKDWNhk+g35RaRFBvJBVPSz15YVQhJY/Tc80qpkBRQoheRVSLyqYjkici9PpZPEJE3RWSviGwVkWyvZf8hIvtF5KCI/Fxk8C/X1Oz28MaBEi6flUl0pI+X7HJCio64UUqFpm4TvYg4gCeB1cBsYJ2IzO7U7DFggzEmB3gQeNhe9wJgGZADzAUWAZf0W/QB2nG0nKqGFt9lG9Ax9EqpkBZIj34xkGeMOWaMaQKeA9Z0ajMbeNOe3uK13ACxQDQQA0QBJX0Nuqc25RYTH+3g4ukZZy/0eKDqlI6hV0qFrEAS/VigwOux057nbQ+w1p6+DkgSkTRjzA6sxF9k314zxhzs/AQicqeI7BSRnaWlpT19DV1yewyvHyhmxcxRxEb5qMHXngZPs/bolVIhK5BE76umbjo9/g5wiYh8glWaKQRaRGQqMAvIxvpw+IyIXHzWxoz5jTFmoTFmYUaGj153H3x0vIKymqaOFwD3pkMrlVIhLpBLCTqBcV6Ps4FT3g2MMaeA6wFEJBFYa4xxicidwPvGmBp72SbgfOCdfog9IK/mFhMTGcGKGaN8N9BEr5QKcYH06D8CponIJBGJBm4GXvJuICLpItK6rfuAp+zpk1g9/UgRicLq7Z9VuhkoHo/h1dxiLp6eQUKMn8+01kSvNXqlVIjqNtEbY1qAbwCvYSXpPxlj9ovIgyJyjd1sOfCpiBwGMoGf2POfB44C+7Dq+HuMMX/v35fg327nGYqrGvyXbcAaQx+VAHGp/tsopdQwFkjpBmPMK8Arnebd7zX9PFZS77yeG/hqH2PstVdzi4lyCJfO6uIXr64Cawz94A/vV0qpQRGyv4w1xrApt4gLpqSTEhflv6FLryyllAptIZvo95+qoqCivuuyDVg1eq3PK6VCWMgm+ldzi4kQuHx2F2WblkZrHH3KOP9tlFJqmAvZRL8pt4glk9JIS4zx36jtgiPao1dKha6QTPRHSqo5Wlrr/9w2rVx6ZSmlVOgLyUS/KbcYgCvmBFCfB0jWRK+UCl0hm+gXjB9BVkps1w2rWn8Vq6UbpVToCrlEf6K8loNFVazufAFwX1xOiE+DqCsXJpQAABOgSURBVLiBD0wppYIk5BJ9a9mm2/o86Bh6pVRYCMlEP3dsMuNGxnff2OXU+rxSKuSFVKI/daaePQVnAivbgDW8Unv0SqkQF1KJ/tWelG0aXNBYpQdilVIhL+QS/fTMRKZkJHbfWMfQK6XCRMgk+tLqRj46UcGqQMs2OoZeKRUmAjpN8XAQH+3gP9bmsGjiyMBWqNIrSymlwkPIJPqEmEhuXNiDk5O5nCAOSAqgnq+UUsNYyJRuesxVCMljIMIR7EiUUmpAhXGi1/PQK6XCQ/gm+iqn1ueVUmEhPBO9x2Of/kB79Eqp0Beeib62FDzNemUppVRYCM9E3zaGXnv0SqnQF56JXsfQK6XCSHgmepcmeqVU+AjTRF8IUfEQlxrsSJRSasCFaaIvsOrzIsGORCmlBlx4Jno9D71SKoyEZ6J3OXUMvVIqbIRfom9phJoSHUOvlAob4Zfoq05Z9zqGXikVJsIw0euVpZRS4SX8Er2OoVdKhZnwTfRaulFKhYnwTPRxIyE6PtiRKKXUoAi/RK9j6JVSYSb8Er1LLziilAovASV6EVklIp+KSJ6I3Otj+QQReVNE9orIVhHJtuevEJHdXrcGEbm2v19Ej7i0R6+UCi/dJnoRcQBPAquB2cA6EZndqdljwAZjTA7wIPAwgDFmizFmvjFmPvAZoA7Y3I/x90xDFTS69ECsUiqsBNKjXwzkGWOOGWOagOeANZ3azAbetKe3+FgOcAOwyRhT19tg+0zH0CulwlAgiX4sUOD12GnP87YHWGtPXwckiUhapzY3A8/6egIRuVNEdorIztLS0gBC6iUdQ6+UCkOBJHpf5/I1nR5/B7hERD4BLgEKgZa2DYiMBuYBr/l6AmPMb4wxC40xCzMyMgIKvFc00SulwlBkAG2cgPcZwLKBU94NjDGngOsBRCQRWGuMcXk1+RzwojGmuW/h9pHLCRIBiVlBDUMppQZTID36j4BpIjJJRKKxSjAveTcQkXQRad3WfcBTnbaxDj9lm0FVVQhJY8ARyOebUkqFhm4TvTGmBfgGVtnlIPAnY8x+EXlQRK6xmy0HPhWRw0Am8JPW9UVkItY3grf7NfLe0PPQK6XCUEBdW2PMK8Arnebd7zX9PPC8n3WPc/bB2+BwOWHsgmBHoZRSgyp8fhnr8VilGx1Dr5QKM+GT6OvKwN2kV5ZSSoWd8En0LvunAFqjV0qFmTBK9PqrWKVUeAqjRN96wRFN9Eqp8BI+ib6qECLjIH5ksCNRSqlBFT6J3lVg1efF1xkdlFIqdIVRotfz0CulwlMYJXqn1ueVUmEpPBJ9SxPUlGiPXikVlsIj0VefAoyOoVdKhaXwSPQ6hl4pFcbCJNHrGHqlVPgKj0Rf1XplKS3dKKXCT3gkepcT4lIhOiHYkSil1KALn0Sv9XmlVJgKk0RfqPV5pVTYCpNErz16pVT4Cv1E31AFjS49EKuUCluhn+irWsfQ65WllFLhKfQTfeuPpfRasUqpMBUGib71EoJao1dKhafQT/RVhSARkDQ62JEopVRQhH6idzmtJO+IDHYkSikVFOGR6LU+r5QKY+GR6LU+r5QKY6Gd6D0eqDqlY+iVUmEttBN9XRm4G3UMvVIqrIV2om87D7326JVS4Ss8Er3W6JVSYSy0E32VXkJQKaVCO9G7nBAZC/FpwY5EKaWCJvQTffJYEAl2JEopFTShn+i1bKOUCnOhneirCjXRK6XCXugm+pYmqC7WRK+UCnsBJXoRWSUin4pInojc62P5BBF5U0T2ishWEcn2WjZeRDaLyEEROSAiE/sv/C5UFwFGx9ArpcJet4leRBzAk8BqYDawTkRmd2r2GLDBGJMDPAg87LVsA/CoMWYWsBg43R+Bd0vH0CulFBBYj34xkGeMOWaMaQKeA9Z0ajMbeNOe3tK63P5AiDTGvA5gjKkxxtT1S+Td0TH0SikFBJboxwIFXo+d9jxve4C19vR1QJKIpAHTgTMi8oKIfCIij9rfEDoQkTtFZKeI7CwtLe35q/Cl9cpSWrpRSoW5QBK9r0HoptPj7wCXiMgnwCVAIdACRAIX2csXAZOBL521MWN+Y4xZaIxZmJGREXj0XXEVQuwIiEnsn+0ppdQwFchll5yA9+kfs4FT3g2MMaeA6wFEJBFYa4xxiYgT+MQYc8xe9lfgfOB3/RB711xOPWulUkHU3NyM0+mkoaEh2KGElNjYWLKzs4mKigp4nUAS/UfANBGZhNVTvxm4xbuBiKQDFcYYD3Af8JTXuqkikmGMKQU+A+wMOLq+0DH0SgWV0+kkKSmJiRMnIvrr9H5hjKG8vByn08mkSZMCXq/b0o0xpgX4BvAacBD4kzFmv4g8KCLX2M2WA5+KyGEgE/iJva4bq2zzpojswyoD/W/gL6sPXAWa6JUKooaGBtLS0jTJ9yMRIS0trcffkgK6YrYx5hXglU7z7veafh543s+6rwM5PYqqrxqrocGlB2KVCjJN8v2vN/s0NH8Z62odWqk1eqWUCs1EX9X6Yynt0SulVGgmev1VrFJh78yZM/z3f/93r9adOHEiZWVlZ83/9a9/zYYNG/oaGgDLly9n587BGZsSUI1+2HEVAgJJo4MdiVIK+NHf93PgVFW/bnP2mGR++Nk5fpe3Jvqvf/3r/facd911V79tazCFbo8+aTQ4Ah9nqpQKLffeey9Hjx5l/vz5LFq0iOXLl3PDDTcwc+ZMbr31Vozp/LvPjh599FEWL17M4sWLycvLA+CBBx7gscceA6we+fe+9z0WL17M9OnT2bZtG2CNNrr99tuZN28e5557Llu2bAGgvr6em2++mZycHG666Sbq6+vbnmvz5s0sXbqUBQsWcOONN1JTU9Ov+yI0e/RVTq3PKzWEdNXzHig//elPyc3NZffu3WzdupU1a9awf/9+xowZw7Jly9i+fTsXXnih3/WTk5P58MMP2bBhA3fffTcvv/zyWW1aWlr48MMPeeWVV/jRj37EG2+8wZNPPgnAvn37OHToECtXruTw4cP86le/Ij4+nr1797J3714WLFgAQFlZGQ899BBvvPEGCQkJPPLII/zsZz/j/vvvP+v5eit0e/Ran1dKeVm8eDHZ2dlEREQwf/58jh8/3mX7devWtd3v2LHDZ5vrr78egPPOO69te++++y6f//znAZg5cyYTJkzg8OHDvPPOO9x2220A5OTkkJNjjTp///33OXDgAMuWLWP+/PmsX7+eEydO9PXldhB6PXpjrBr9jCuDHYlSagiJiYlpm3Y4HLS0tHTZ3nu8ur+x663b9N5eVyUhX9sxxnD55Zfz7LPPdhlPX4Rej762DNyNOoZeqTCXlJREdXV1r9ffuHFj2/3SpUsDXu/iiy/mmWeeAeDw4cOcPHmSGTNmdJifm5vL3r17ATj//PPZvn1723GAuro6Dh8+3Ou4fQm9Hr2OoVdKAWlpaSxbtoy5c+cSFxdHZmZmj9ZvbGxkyZIleDyeHvW2v/71r3PXXXcxb948IiMjefrpp4mJieFrX/sat99+Ozk5OcyfP5/FixcDkJGRwdNPP826detobGwE4KGHHmL69Ok9ircr0t2R58G2cOFC06expQf/Dhtvgzu3wphz+ysspVQPHTx4kFmzZgU7jJDka9+KyMfGmIW+2ode6ab19AfJejBWKaUgFEs3rgJwxEBCerAjUUoNcddddx35+fkd5j3yyCNcccUVQYpoYIReoq8qtOrzetY8pVQ3XnzxxWCHMChCsHSjY+iVUspbCCb6Qq3PK6WUl9BK9O5mqC7SHr1SSnkJrURfXQQYHUOvlFJeQivR63nolVK2vpyP/vHHH6eurs7v8i9/+cscOHCgt6F1kJiY2C/b6UpojbrRMfRKDU2b7oXiff27zax5sPqnfhf35Xz0jz/+OLfddhvx8fFnLXO73fz2t7/t8TaDKcR69AXWvZZulAp7vT0f/c9//nNOnTrFihUrWLFiBWD1uu+//36WLFnCjh07OlwdKjExke9///ucc845nH/++ZSUlABw4sQJLr30UnJycrj00ks5efIkAPn5+SxdupRFixbxgx/8oMNzP/rooyxatIicnBx++MMf9t/OMMYMqdt5551neu3lbxvz8Ljer6+U6jcHDhwI6vPn5+ebOXPmGGOM2bJli0lOTjYFBQXG7Xab888/32zbts3vuhMmTDClpaVtjwGzcePGtseXXHKJ+eijj9qWvfTSS8YYY7773e+aH//4x8YYY66++mrz9NNPG2OM+d3vfmfWrFljjDHms5/9rFm/fr0xxphf/vKXJiEhwRhjzGuvvWa+8pWvGI/HY9xut7nqqqvM22+/7TM+X/sW2Gn85NUQ69E79ayVSimfeno+em8Oh4O1a9f6XBYdHc3VV18NdDwv/Y4dO7jlllsA+PznP8+7774LwPbt29vOdd963nqwrjK1efNmzj33XBYsWMChQ4c4cuRIT1+mT6FXo0/Wso1S6mw9PR+9t9jYWBwOh89lUVFRbeeZ72q73Z3f3hjDfffdx1e/+tWA4wpUiPXoC3TEjVIK6Nv56Pt6LnuACy64gOeeew6AZ555pu2yhcuWLeswv9UVV1zBU0891Xa92MLCQk6fPt2nGFqFTo++sQYazuiBWKUU0Lfz0d95552sXr2a0aNHt13cu6d+/vOfc8cdd/Doo4+SkZHB73//ewCeeOIJbrnlFp544okO5aCVK1dy8ODBtoucJCYm8oc//IFRo0b16vm9hc756GvLYdM9MP8WmHpp/wemlOoRPR/9wOnp+ehDp0efkAY3/C7YUSil1JATOoleKaV6SM9Hr5RSfWSM8TnCZKgYjuej7025PbRG3SilhozY2FjKy8t7lZiUb8YYysvLiY2N7dF62qNXSg2I7OxsnE4npaWlwQ4lpMTGxpKd3bNh5JrolVIDIioqikmTJgU7DIWWbpRSKuRpoldKqRCniV4ppULckPtlrIiUAif6sIl0oKyfwhkIGl/faHx9o/H1zVCOb4IxJsPXgiGX6PtKRHb6+xnwUKDx9Y3G1zcaX98M9fj80dKNUkqFOE30SikV4kIx0f8m2AF0Q+PrG42vbzS+vhnq8fkUcjV6pZRSHYVij14ppZQXTfRKKRXihmWiF5FVIvKpiOSJyL0+lseIyEZ7+QciMnEQYxsnIltE5KCI7BeRb/pos1xEXCKy277dP1jxecVwXET22c9/1iW9xPJzex/uFZEFgxjbDK99s1tEqkTk7k5tBnUfishTInJaRHK95o0UkddF5Ih9n+pn3S/abY6IyBcHMb5HReSQ/fd7UURG+Fm3y/fCAMb3gIgUev0Nr/Szbpf/7wMY30av2I6LyG4/6w74/uszY8ywugEO4CgwGYgG9gCzO7X5OvBre/pmYOMgxjcaWGBPJwGHfcS3HHg5yPvxOJDexfIrgU2AAOcDHwTx712M9WOQoO1D4GJgAZDrNe8/gHvt6XuBR3ysNxI4Zt+n2tOpgxTfSiDSnn7EV3yBvBcGML4HgO8E8Pfv8v99oOLrtPw/gfuDtf/6ehuOPfrFQJ4x5pgxpgl4DljTqc0aYL09/TxwqQzS1Q+MMUXGmF32dDVwEBiOVyxfA2wwlveBESIyOghxXAocNcb05dfSfWaMeQeo6DTb+322HrjWx6pXAK8bYyqMMZXA68CqwYjPGLPZGNNiP3wf6Nm5bfuRn/0XiED+3/usq/js3PE54Nn+ft7BMhwT/VigwOuxk7MTaVsb+43uAtIGJTovdsnoXOADH4uXisgeEdkkInMGNTCLATaLyMcicqeP5YHs58FwM/7/wYK9DzONMUVgfcADo3y0GSr78Q6sb2i+dPdeGEjfsEtLT/kpfQ2F/XcRUGKMOeJneTD3X0CGY6L31TPvPEY0kDYDSkQSgb8Adxtjqjot3oVVijgH+AXw18GMzbbMGLMAWA38o4hc3Gn5UNiH0cA1wJ99LB4K+zAQQ2E/fh9oAZ7x06S798JA+RUwBZgPFGGVRzoL+v4D1tF1bz5Y+y9gwzHRO4FxXo+zgVP+2ohIJJBC77429oqIRGEl+WeMMS90Xm6MqTLG1NjTrwBRIpI+WPHZz3vKvj8NvIj1FdlbIPt5oK0GdhljSjovGAr7EChpLWfZ96d9tAnqfrQP/l4N3GrsgnJnAbwXBoQxpsQY4zbGeID/9fO8wd5/kcD1wEZ/bYK1/3piOCb6j4BpIjLJ7vHdDLzUqc1LQOvohhuAt/y9yfubXc/7HXDQGPMzP22yWo8ZiMhirL9D+WDEZz9ngogktU5jHbTL7dTsJeAL9uib8wFXa5liEPntSQV7H9q832dfBP7mo81rwEoRSbVLEyvteQNORFYB3wOuMcbU+WkTyHthoOLzPuZznZ/nDeT/fSBdBhwyxjh9LQzm/uuRYB8N7s0Na0TIYayj8d+35z2I9YYGiMX6up8HfAhMHsTYLsT6arkX2G3frgTuAu6y23wD2I81guB94IJB3n+T7efeY8fRug+9YxTgSXsf7wMWDnKM8ViJO8VrXtD2IdYHThHQjNXL/Aes4z5vAkfs+5F224XAb73WvcN+L+YBtw9ifHlY9e3W92HrSLQxwCtdvRcGKb7/s99be7GS9+jO8dmPz/p/H4z47PlPt77nvNoO+v7r601PgaCUUiFuOJZulFJK9YAmeqWUCnGa6JVSKsRpoldKqRCniV4ppUKcJnqllApxmuiVUirE/X8Cw2aevQe4XgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tn_loss = tn_hist.history['loss']\n",
    "tn_acc = tn_hist.history['accuracy']\n",
    "\n",
    "tn_loss2 = tn_hist2.history['loss']\n",
    "tn_acc2 = tn_hist2.history['accuracy']\n",
    "\n",
    "plt.plot(tn_loss, label='tn_binode')\n",
    "plt.plot(tn_loss2, label='tn_trinode')\n",
    "plt.title('Loss comparsion in training')\n",
    "plt.ylim(0, 0.15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tn_acc, label='tn_binode')\n",
    "plt.plot(tn_acc2, label='tn_trinode')\n",
    "plt.title('Accuracy comparsion in training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tensor Networks in Neural Networks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
