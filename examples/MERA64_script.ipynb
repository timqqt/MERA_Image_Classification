{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 16, 16, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "grid_mer_ain16 (GridMERAin16)   (None, 4)            1114436     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           grid_mer_ain16[0][0]             \n",
      "                                                                 grid_mer_ain16[1][0]             \n",
      "                                                                 grid_mer_ain16[2][0]             \n",
      "                                                                 grid_mer_ain16[3][0]             \n",
      "                                                                 grid_mer_ain16[4][0]             \n",
      "                                                                 grid_mer_ain16[5][0]             \n",
      "                                                                 grid_mer_ain16[6][0]             \n",
      "                                                                 grid_mer_ain16[7][0]             \n",
      "                                                                 grid_mer_ain16[8][0]             \n",
      "                                                                 grid_mer_ain16[9][0]             \n",
      "                                                                 grid_mer_ain16[10][0]            \n",
      "                                                                 grid_mer_ain16[11][0]            \n",
      "                                                                 grid_mer_ain16[12][0]            \n",
      "                                                                 grid_mer_ain16[13][0]            \n",
      "                                                                 grid_mer_ain16[14][0]            \n",
      "                                                                 grid_mer_ain16[15][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            130         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,114,566\n",
      "Trainable params: 1,114,566\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-1-5e1820ca2238>:349: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 188 steps, validate for 32 steps\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 396s 2s/step - loss: 0.6932 - accuracy: 0.4933 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 140s 744ms/step - loss: 0.6932 - accuracy: 0.4887 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 141s 752ms/step - loss: 0.6932 - accuracy: 0.4913 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 141s 748ms/step - loss: 0.6932 - accuracy: 0.4980 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 141s 751ms/step - loss: 0.6932 - accuracy: 0.4977 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 141s 751ms/step - loss: 0.6932 - accuracy: 0.4913 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 142s 755ms/step - loss: 0.6932 - accuracy: 0.4940 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 140s 746ms/step - loss: 0.6932 - accuracy: 0.4963 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - 141s 752ms/step - loss: 0.6932 - accuracy: 0.4827 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - 142s 754ms/step - loss: 0.6932 - accuracy: 0.4913 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 11/100\n",
      " 25/188 [==>...........................] - ETA: 1:51 - loss: 0.6931 - accuracy: 0.5306"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e1820ca2238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;31m# TensorNetwork model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0mMERA_model_64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m \u001b[0mMERA_model_64_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMERA_model_64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;31m# TN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import pandas as pd\n",
    "import os \n",
    "tf.compat.v1.enable_v2_behavior\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"tensorflow\")\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "class ClutteredMNISTDataset(Sequence):\n",
    "    reg_dataset_size = 11276\n",
    "\n",
    "    def __init__(self, base_path, csv_path, data_scaling=1., num_examples=None, balance=None, num_classes=2):\n",
    "        self.base_path = base_path\n",
    "        self.csv_path = csv_path\n",
    "        self.csv = pd.read_csv(csv_path)\n",
    "        self.data_scaling = data_scaling\n",
    "        self.num_examples = num_examples\n",
    "        self.balance = balance\n",
    "        self.num_classes = num_classes\n",
    "        self.img_paths = self.csv['img_path'].values\n",
    "        self.lbls = self.csv['label'].values.astype(np.int32)\n",
    "        permute_idx = np.random.permutation(len(self.img_paths))\n",
    "        self.img_paths = self.img_paths[permute_idx]\n",
    "        self.lbls = self.lbls[permute_idx]\n",
    "        self.weights = np.ones([len(self.img_paths), ])\n",
    "\n",
    "        if self.num_examples is not None and self.balance is not None:\n",
    "            # only rebalance if examples and balance is given\n",
    "            assert self.num_examples <= len(self.img_paths), \\\n",
    "                'not enough examples in dataset {} - {}'.format(self.num_examples, len(self.img_paths))\n",
    "\n",
    "            pos_num = int(self.balance * self.num_examples)\n",
    "            neg_num = self.num_examples - pos_num\n",
    "\n",
    "            pos_mask = (self.lbls == 1)\n",
    "            pos_paths = self.img_paths[pos_mask][:pos_num]\n",
    "\n",
    "            neg_mask = (self.lbls == 0)\n",
    "            neg_paths = self.img_paths[neg_mask][:neg_num]\n",
    "\n",
    "            self.img_paths = np.concatenate([pos_paths, neg_paths], 0)\n",
    "            self.lbls = np.concatenate([np.ones([pos_num, ]), np.zeros([neg_num, ])], 0)\n",
    "            self.weights = np.ones([self.num_examples, ])\n",
    "            self.weights[:pos_num] /= pos_num\n",
    "            self.weights[pos_num:] /= neg_num\n",
    "\n",
    "        self.shrinkage = self.reg_dataset_size // len(self.img_paths)\n",
    "        permute_idx = np.random.permutation(len(self.img_paths))\n",
    "        self.img_paths = self.img_paths[permute_idx]\n",
    "        self.lbls = self.lbls[permute_idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = os.path.join(self.base_path, self.img_paths[index])\n",
    "        lbl = self.lbls[index].astype(np.int64)\n",
    "        lbl = np.eye(self.num_classes)[lbl].astype(np.int64)\n",
    "        img = np.load(path)\n",
    "        if isinstance(img, np.lib.npyio.NpzFile):\n",
    "            img = img['arr_0']\n",
    "\n",
    "        if self.data_scaling != 1.:\n",
    "            img = zoom(img, self.data_scaling)\n",
    "            img = img.clip(0., 1.)\n",
    "\n",
    "        img = (img[:,:,np.newaxis].astype(np.float32).astype(np.float32) -0.5)/1\n",
    "\n",
    "        return img, lbl\n",
    "    \n",
    "\n",
    "class Batcher(Sequence):\n",
    "    \"\"\"Assemble a sequence of things into a sequence of batches.\"\"\"\n",
    "    def __init__(self, sequence, batch_size=16):\n",
    "        self._batch_size = batch_size\n",
    "        self._sequence = sequence\n",
    "        self._idxs = np.arange(len(self._sequence))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self._sequence) / self._batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        start = i*self._batch_size\n",
    "        end = min(len(self._sequence), start+self._batch_size)\n",
    "        data = [self._sequence[j] for j in self._idxs[start:end]]\n",
    "        inputs = [d[0] for d in data]\n",
    "        outputs = [d[1] for d in data]\n",
    "\n",
    "        return self._stack(inputs), self._stack(outputs)\n",
    "\n",
    "    def _stack(self, data):\n",
    "        if data is None:\n",
    "            return None\n",
    "\n",
    "        if not isinstance(data[0], (list, tuple)):\n",
    "            return np.stack(data)\n",
    "\n",
    "        seq = type(data[0])\n",
    "        K = len(data[0])\n",
    "        data = seq(\n",
    "            np.stack([d[k] for d in data])\n",
    "            for k in range(K)\n",
    "        )\n",
    "\n",
    "        return data\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self._idxs)\n",
    "        self._sequence.on_epoch_end()\n",
    "        \n",
    "        \n",
    "        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_v2_behavior\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tn.set_default_backend(\"tensorflow\")\n",
    "\n",
    "data_path = '/datacommons/carin/fk43/NeedleinHaystack/mnist64_1/'\n",
    "train_data=ClutteredMNISTDataset(base_path=data_path, csv_path=os.path.join(data_path, 'train.csv'), data_scaling=1., num_examples=6000, balance=0.5)\n",
    "val_data = ClutteredMNISTDataset(base_path=data_path, csv_path=os.path.join(data_path, 'val.csv'), data_scaling=1., num_examples=1000, balance=0.5)\n",
    "test_data = ClutteredMNISTDataset(base_path=data_path, csv_path=os.path.join(data_path, 'test.csv'), data_scaling=1., num_examples=1000, balance=0.5)\n",
    "\n",
    "\n",
    "train_data = Batcher(train_data, batch_size=batch_size)\n",
    "val_data = Batcher(val_data, batch_size=batch_size)\n",
    "test_data = Batcher(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "class GridMERAin16(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, kernel_dims, bond_dims, output_dims):\n",
    "        super(GridMERAin16, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        in_dims = int((kernel_dims//4)**2)\n",
    "        self.entanglers = []\n",
    "        self.isometries= []\n",
    "        self.kernel_dims = kernel_dims\n",
    "        self.output_dims = output_dims\n",
    "        #entanglers\n",
    "        self.entanglers1 = tf.Variable(tf.random.normal\n",
    "                                             (shape=(in_dims, in_dims, \n",
    "                                                     in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=1/10000), \n",
    "                                              trainable=True)\n",
    "        self.entanglers2 = tf.Variable(tf.random.normal\n",
    "                                             (shape=(bond_dims, bond_dims, \n",
    "                                                     bond_dims, bond_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=1/10000), \n",
    "                                              trainable=True)\n",
    "        # isometries\n",
    "        self.isometries1 = [tf.Variable(tf.random.normal(shape=(in_dims, in_dims, in_dims, \n",
    "                                                                            bond_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/100000),\n",
    "                                            trainable=True), \n",
    "                           tf.Variable(tf.random.normal(shape=(in_dims, in_dims, bond_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/100000),\n",
    "                                            trainable=True),\n",
    "                           tf.Variable(tf.random.normal(shape=(in_dims, bond_dims, in_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/100000),\n",
    "                                            trainable=True),\n",
    "                           tf.Variable(tf.random.normal(shape=(bond_dims, in_dims, in_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/100000),\n",
    "                                            trainable=True)]\n",
    "        \n",
    "        self.isometries2 = tf.Variable(tf.random.normal(shape=(bond_dims, bond_dims, bond_dims, \n",
    "                                                                            bond_dims, output_dims)\n",
    "                                                                     , stddev=1.0/100000),\n",
    "                                            trainable=True)\n",
    "\n",
    "        #print(self.final_mps.shape)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(output_dims,)), name=\"bias\", trainable=True)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, entanglers1, entanglers2, isometries1, isometries2, bias_var, kernel_dims):\n",
    "            input_vv = []\n",
    "            step = int(kernel_dims//4)\n",
    "            for i in range(4):\n",
    "                for ii in range(4):\n",
    "                    input_vv.append(tf.reshape(input_vec[i*step:i*step+step, ii*step:ii*step+step, 0], (1, step**2)))\n",
    "            input_vec = tf.concat(input_vv, axis=0)\n",
    "            input_vec = tf.reshape(input_vec, (16, step**2))\n",
    "            input_vec = tf.unstack(input_vec)\n",
    "            input_nodes = []\n",
    "            for e_iv in input_vec:\n",
    "                input_nodes.append(tn.Node(e_iv))\n",
    "            \n",
    "            e_nodes1 = tn.Node(entanglers1)\n",
    "            e_nodes2 = tn.Node(entanglers2)\n",
    "                \n",
    "                                     \n",
    "            isometries_nodes1 = []\n",
    "            for eiso in isometries1:\n",
    "                isometries_nodes1.append(tn.Node(eiso))\n",
    "            isometries_nodes2 = tn.Node(isometries2)\n",
    "            \n",
    "            \n",
    "            e_nodes1[0] ^ input_nodes[5][0]\n",
    "            e_nodes1[1] ^ input_nodes[6][0]\n",
    "            e_nodes1[2] ^ input_nodes[9][0]\n",
    "            e_nodes1[3] ^ input_nodes[10][0]\n",
    "\n",
    "            e_nodes1[4] ^ isometries_nodes1[0][3]\n",
    "            e_nodes1[5] ^ isometries_nodes1[1][2]\n",
    "            e_nodes1[6] ^ isometries_nodes1[2][1]\n",
    "            e_nodes1[7] ^ isometries_nodes1[3][0]     \n",
    "            \n",
    "            input_nodes[0][0] ^ isometries_nodes1[0][0]\n",
    "            input_nodes[1][0] ^ isometries_nodes1[0][1]\n",
    "            input_nodes[4][0] ^ isometries_nodes1[0][2]\n",
    "            \n",
    "            input_nodes[2][0] ^ isometries_nodes1[1][0]\n",
    "            input_nodes[3][0] ^ isometries_nodes1[1][1]\n",
    "            input_nodes[7][0] ^ isometries_nodes1[1][3]\n",
    "            \n",
    "            input_nodes[8][0] ^ isometries_nodes1[2][0]\n",
    "            input_nodes[12][0] ^ isometries_nodes1[2][2]\n",
    "            input_nodes[13][0] ^ isometries_nodes1[2][3]\n",
    "            \n",
    "            input_nodes[11][0] ^ isometries_nodes1[3][1]\n",
    "            input_nodes[14][0] ^ isometries_nodes1[3][2]\n",
    "            input_nodes[15][0] ^ isometries_nodes1[3][3]\n",
    "            \n",
    "            \n",
    "            isometries_nodes1[0][4] ^ e_nodes2[0]\n",
    "            isometries_nodes1[1][4] ^ e_nodes2[1]\n",
    "            isometries_nodes1[2][4] ^ e_nodes2[2]\n",
    "            isometries_nodes1[3][4] ^ e_nodes2[3]\n",
    "\n",
    "            e_nodes2[4] ^ isometries_nodes2[0]\n",
    "            e_nodes2[5] ^ isometries_nodes2[1]\n",
    "            e_nodes2[6] ^ isometries_nodes2[2]\n",
    "            e_nodes2[7] ^ isometries_nodes2[3]\n",
    "\n",
    "                            \n",
    "            nodes = tn.reachable(isometries_nodes2)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            #result = (c @ b).tensor\n",
    "            # Finally, add bias.\n",
    "            #return result + bias_var\n",
    "            return result\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        output = tf.vectorized_map(lambda vec: f(vec, self.entanglers1, self.entanglers2,\n",
    "                                                 self.isometries1,  self.isometries2, self.bias, self.kernel_dims), inputs)\n",
    "        return tf.reshape(output, (-1, self.output_dims))\n",
    "    \n",
    "from tensorflow.keras.layers import Lambda, Input, Concatenate, Reshape, Softmax, Dense, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "def get_model(input_shape=(64, 64, 1)):\n",
    "    x_in = Input(shape=input_shape)\n",
    "    x_out_list = []\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            subx = Lambda(lambda x:x[:, 8*i:8*(i+1),8*j:8*(j+1),:] )(x_in)\n",
    "            x_out_list.append(GridMERAin16(kernel_dims=8, bond_dims=2, output_dims=1)(subx))\n",
    "    x_out = Concatenate(axis=1)(x_out_list)\n",
    "    x_out = Reshape(target_shape=(8, 8, 1))(x_out)\n",
    "    y = GridMERAin16(kernel_dims=8, bond_dims=2, output_dims=2)(x_out)\n",
    "    y = Softmax()(y)\n",
    "    return Model(inputs=x_in, outputs=y)\n",
    "\n",
    "def get_dense_model(input_shape=(64, 64, 1)):\n",
    "    x_in = Input(shape=input_shape)\n",
    "    x_out_list = []\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            subx = Lambda(lambda x:x[:, 8*i:8*(i+1),8*j:8*(j+1),:] )(x_in)\n",
    "            x_out_list.append(GridMERAin16(kernel_dims=8, bond_dims=2, output_dims=2)(subx))\n",
    "    x_out = Concatenate(axis=1)(x_out_list)\n",
    "    x_out = Flatten()(x_out)\n",
    "    y = Dense(2, activation='softmax')(x_out)\n",
    "    #x_out = Reshape(target_shape=(8, 8, 1))(x_out)\n",
    "    #y = GridMERAin16(kernel_dims=8, bond_dims=2, output_dims=2)(x_out)\n",
    "    #y = Softmax()(y)\n",
    "    return Model(inputs=x_in, outputs=y)\n",
    "\n",
    "def get_loop_model(input_shape=(64, 64, 1)):\n",
    "    x_in = Input(shape=input_shape)\n",
    "    x_out_list = []\n",
    "    MERA_layer = GridMERAin16(kernel_dims=16, bond_dims=2, output_dims=4)\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            subx = Lambda(lambda x:x[:, 16*i:16*(i+1),16*j:16*(j+1),:] )(x_in)\n",
    "            x_out_list.append(MERA_layer(subx))\n",
    "    x_out = Concatenate(axis=1)(x_out_list)\n",
    "    x_out = Flatten()(x_out)\n",
    "    y = Dense(2, activation='softmax')(x_out)\n",
    "    #x_out = Reshape(target_shape=(8, 8, 1))(x_out)\n",
    "    #y = GridMERAin16(kernel_dims=8, bond_dims=2, output_dims=2)(x_out)\n",
    "    #y = Softmax()(y)\n",
    "    return Model(inputs=x_in, outputs=y)\n",
    "\n",
    "def get_deep_model(input_shape=(64, 64, 1)):\n",
    "    x_in = Input(shape=input_shape)\n",
    "    x_in2_list = []\n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "            subx = Lambda(lambda x:x[:, 4*i:4*(i+1),4*j:4*(j+1),:] )(x_in)\n",
    "            x_in2_list.append(GridMERAin16(kernel_dims=1, bond_dims=2, output_dims=1)(subx))\n",
    "    x_in2 = Concatenate(axis=1)(x_in2_list)\n",
    "    x_in2 = Reshape(target_shape=(16, 16, 1))(x_in2)\n",
    "    x_out_list = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            subx = Lambda(lambda x:x[:, 4*i:4*(i+1),4*j:4*(j+1),:] )(x_in2)\n",
    "            x_out_list.append(GridMERAin16(kernel_dims=1, bond_dims=2, output_dims=1)(subx))\n",
    "    x_out = Concatenate(axis=1)(x_out_list)\n",
    "    x_out = Reshape(target_shape=(4, 4, 1))(x_out)\n",
    "    y = GridMERAin16(kernel_dims=1, bond_dims=2, output_dims=2)(x_out)\n",
    "    y = Softmax()(y)\n",
    "    return Model(inputs=x_in, outputs=y)\n",
    "\n",
    "\n",
    "MERA_model_64 = get_deep_model(input_shape=(64, 64, 1))\n",
    "\n",
    "MERA_model_64.summary()\n",
    "\n",
    "# TensorNetwork model\n",
    "MERA_model_64.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=['accuracy'])\n",
    "MERA_model_64_hist = MERA_model_64.fit_generator(train_data, validation_data=val_data, epochs=100, verbose=1)\n",
    "\n",
    "# TN model\n",
    "MERA_model_64.evaluate_generator(test_data, int(np.ceil(1000/batch_size)), workers = 1)\n",
    "\n",
    "tn_loss = MERA_model_64_hist.history['loss']\n",
    "tn_acc = MERA_model_64_hist.history['accuracy']\n",
    "\n",
    "\n",
    "np.savetxt('loss_MERA64.out', tn_loss, delimiter=',')  \n",
    "np.savetxt('acc_MERA64.out', tn_acc, delimiter=',')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERA_model_64.evaluate_generator(test_data, int(np.ceil(1000/batch_size)),  workers = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MERA_model_64_hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
