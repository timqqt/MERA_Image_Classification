{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_v2_behavior\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"tensorflow\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(\"/hpc/group/carin/fk43/FanjieKong/Megapixels/new_tfquantum/quantum/TensorNetwork/mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype(np.float32)\n",
    "y_train = to_categorical(y_train, 10).astype(np.float32)\n",
    "x_test = x_test.reshape((10000, 28, 28, 1))\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# print(x_train.shape, y_train.shape)\n",
    "# print(x_test.shape, y_test.shape)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_train = (x_train-128)/255\n",
    "xxx_test = (x_test-128)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_train = (tf.image.resize(x_train, [16,16]).numpy()-128)/255\n",
    "xx_test = (tf.image.resize(x_test, [16,16]).numpy()-128)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd27df9c290>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xx_train[12000,2:-2, 2:-2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoDMERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, bond_dims, output_dims, n_layers=None):\n",
    "        super(twoDMERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        if n_layers is None:\n",
    "            n_layers = np.floor(np.log2(input_dim))\n",
    "        self.n_layers = n_layers\n",
    "        in_dims = 9\n",
    "        dims = input_dim\n",
    "        self.entanglers = []\n",
    "        self.isometries= []\n",
    "        for i in range(n_layers):\n",
    "            if dims % 2 == 0:\n",
    "                dims = int(dims//2)\n",
    "                self.entanglers.append([tf.Variable(tf.random.normal\n",
    "                                             (shape=(in_dims, in_dims, \n",
    "                                                     in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=2.0), \n",
    "                                              trainable=True) for j in range(dims**2)])\n",
    "                in_dims = bond_dims\n",
    "                self.isometries.append([tf.Variable(tf.random.normal(shape=(bond_dims, bond_dims, bond_dims, \n",
    "                                                                            bond_dims, bond_dims)\n",
    "                                                                     , stddev=2.0),\n",
    "                                            trainable=True) for j in range(dims**2)])\n",
    "                \n",
    "                \n",
    "                \n",
    "        self.final_mps = tf.Variable(tf.random.normal(shape=(bond_dims,)*(dims**2) + (output_dims,)\n",
    "                                                                 , stddev=2.0))\n",
    "\n",
    "        #print(self.final_mps.shape)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(output_dims,)), name=\"bias\", trainable=True)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, entanglers, isometries, final_mps, bias_var, n_layers):\n",
    "            input_vec = input_vec[2:-2, 2:-2, :]\n",
    "            input_vv = []\n",
    "            for i in range(8):\n",
    "                for ii in range(8):\n",
    "                    input_vv.append(tf.reshape(input_vec[i*3:i*3+3, ii*3:ii*3+3, 0], (1, 9)))\n",
    "            input_vec = tf.concat(input_vv, axis=0)\n",
    "            input_vec = tf.reshape(input_vec, (64, 9))\n",
    "            input_vec = tf.unstack(input_vec)\n",
    "            #print(input_vec)\n",
    "            input_nodes = []\n",
    "            for e_iv in input_vec:\n",
    "#                 input_nodes.append(tn.Node(tf.concat([tf.sin(e_iv), tf.cos(e_iv)], axis=0)))\n",
    "                input_nodes.append(tn.Node(e_iv))\n",
    "\n",
    "            entangler_nodes_list = []\n",
    "            for layer in range(self.n_layers):\n",
    "                entangler_nodes_list_in_one_layer = []\n",
    "                for idx, e_e in enumerate(entanglers[layer]):\n",
    "                    entangler_nodes_list_in_one_layer.append(tn.Node(e_e))\n",
    "                entangler_nodes_list.append(entangler_nodes_list_in_one_layer)\n",
    "                                     \n",
    "            isometries_nodes_list = []\n",
    "            for layer in range(self.n_layers):\n",
    "                isometries_nodes_list_in_one_layer = []\n",
    "                for idx, e_i in enumerate(isometries[layer]):\n",
    "                    isometries_nodes_list_in_one_layer.append(tn.Node(e_i))\n",
    "                isometries_nodes_list.append(isometries_nodes_list_in_one_layer)\n",
    "            \n",
    "            mps_node = tn.Node(final_mps)\n",
    "            \n",
    "            \n",
    "            # Layer 1\n",
    "            for i in range(4):\n",
    "                for ii in range(4):\n",
    "                    entangler_nodes_list[0][4*i+ii][0] ^ input_nodes[0+16*i+2*ii][0]\n",
    "                    entangler_nodes_list[0][4*i+ii][1] ^ input_nodes[1+16*i+2*ii][0]\n",
    "                    entangler_nodes_list[0][4*i+ii][2] ^ input_nodes[8+16*i+2*ii][0]\n",
    "                    entangler_nodes_list[0][4*i+ii][3] ^ input_nodes[9+16*i+2*ii][0]\n",
    "            \n",
    "                    entangler_nodes_list[0][4*i+ii][4] ^ isometries_nodes_list[0][4*i+ii][0]\n",
    "                    entangler_nodes_list[0][4*i+ii][5] ^ isometries_nodes_list[0][4*i+ii][1]\n",
    "                    entangler_nodes_list[0][4*i+ii][6] ^ isometries_nodes_list[0][4*i+ii][2]\n",
    "                    entangler_nodes_list[0][4*i+ii][7] ^ isometries_nodes_list[0][4*i+ii][3]\n",
    "            \n",
    "            #layer 2\n",
    "            for i in range(2):\n",
    "                for ii in range(2):\n",
    "                    entangler_nodes_list[1][2*i+ii][0] ^ isometries_nodes_list[0][0+8*i+2*ii][4]\n",
    "                    entangler_nodes_list[1][2*i+ii][1] ^ isometries_nodes_list[0][1+8*i+2*ii][4]\n",
    "                    entangler_nodes_list[1][2*i+ii][2] ^ isometries_nodes_list[0][4+8*i+2*ii][4]\n",
    "                    entangler_nodes_list[1][2*i+ii][3] ^ isometries_nodes_list[0][5+8*i+2*ii][4]\n",
    "            \n",
    "                    entangler_nodes_list[1][2*i+ii][4] ^ isometries_nodes_list[1][2*i+ii][0]\n",
    "                    entangler_nodes_list[1][2*i+ii][5] ^ isometries_nodes_list[1][2*i+ii][1]\n",
    "                    entangler_nodes_list[1][2*i+ii][6] ^ isometries_nodes_list[1][2*i+ii][2]\n",
    "                    entangler_nodes_list[1][2*i+ii][7] ^ isometries_nodes_list[1][2*i+ii][3]\n",
    "                    isometries_nodes_list[1][2*i+ii][4] ^ mps_node[2*i+ii] \n",
    "                    \n",
    "            nodes = tn.reachable(mps_node)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            #result = (c @ b).tensor\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        output = tf.vectorized_map(lambda vec: f(vec, self.entanglers, \n",
    "                                                 self.isometries, self.final_mps, self.bias, self.n_layers), inputs)\n",
    "        return tf.reshape(output, (-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid4DMERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, bond_dims, output_dims, n_layers=None):\n",
    "        super(Grid4DMERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        if n_layers is None:\n",
    "            n_layers = np.floor(np.log2(input_dim))\n",
    "        self.n_layers = n_layers\n",
    "        in_dims = 16\n",
    "        dims = input_dim\n",
    "        self.entanglers = []\n",
    "        self.isometries= []\n",
    "        \n",
    "        #entanglers\n",
    "        self.entanglers1 = tf.Variable(tf.random.normal\n",
    "                                             (shape=(in_dims, in_dims, \n",
    "                                                     in_dims, in_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=1.0/10000), \n",
    "                                              trainable=True)\n",
    "        self.entanglers2 = tf.Variable(tf.random.normal\n",
    "                                             (shape=(bond_dims, bond_dims, \n",
    "                                                     bond_dims, bond_dims, bond_dims, bond_dims, bond_dims, bond_dims),\n",
    "                                              stddev=1.0/10000), \n",
    "                                              trainable=True)\n",
    "        # isometries\n",
    "        self.isometries1 = [tf.Variable(tf.random.normal(shape=(in_dims, in_dims, in_dims, \n",
    "                                                                            bond_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/10*10000),\n",
    "                                            trainable=True), \n",
    "                           tf.Variable(tf.random.normal(shape=(in_dims, in_dims, bond_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/10*10000),\n",
    "                                            trainable=True),\n",
    "                           tf.Variable(tf.random.normal(shape=(in_dims, bond_dims, in_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/10*10000),\n",
    "                                            trainable=True),\n",
    "                           tf.Variable(tf.random.normal(shape=(bond_dims, in_dims, in_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/10*10000),\n",
    "                                            trainable=True)]\n",
    "        \n",
    "        self.isometries2 = tf.Variable(tf.random.normal(shape=(bond_dims, bond_dims, bond_dims, \n",
    "                                                                            bond_dims, output_dims)\n",
    "                                                                     , stddev=1.0/10*10000),\n",
    "                                            trainable=True)\n",
    "\n",
    "        #print(self.final_mps.shape)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(output_dims,)), name=\"bias\", trainable=True)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, entanglers1, entanglers2, isometries1, isometries2, bias_var, n_layers):\n",
    "            input_vv = []\n",
    "            for i in range(4):\n",
    "                for ii in range(4):\n",
    "                    input_vv.append(tf.reshape(input_vec[i*4:i*4+4, ii*4:ii*4+4, 0], (1, 16)))\n",
    "            input_vec = tf.concat(input_vv, axis=0)\n",
    "            input_vec = tf.reshape(input_vec, (16, 16))\n",
    "            input_vec = tf.unstack(input_vec)\n",
    "            input_nodes = []\n",
    "            for e_iv in input_vec:\n",
    "                input_nodes.append(tn.Node(e_iv))\n",
    "            \n",
    "            e_nodes1 = tn.Node(entanglers1)\n",
    "            e_nodes2 = tn.Node(entanglers2)\n",
    "                \n",
    "                                     \n",
    "            isometries_nodes1 = []\n",
    "            for eiso in isometries1:\n",
    "                isometries_nodes1.append(tn.Node(eiso))\n",
    "            isometries_nodes2 = tn.Node(isometries2)\n",
    "            \n",
    "            \n",
    "            e_nodes1[0] ^ input_nodes[5][0]\n",
    "            e_nodes1[1] ^ input_nodes[6][0]\n",
    "            e_nodes1[2] ^ input_nodes[9][0]\n",
    "            e_nodes1[3] ^ input_nodes[10][0]\n",
    "\n",
    "            e_nodes1[4] ^ isometries_nodes1[0][3]\n",
    "            e_nodes1[5] ^ isometries_nodes1[1][2]\n",
    "            e_nodes1[6] ^ isometries_nodes1[2][1]\n",
    "            e_nodes1[7] ^ isometries_nodes1[3][0]     \n",
    "            \n",
    "            input_nodes[0][0] ^ isometries_nodes1[0][0]\n",
    "            input_nodes[1][0] ^ isometries_nodes1[0][1]\n",
    "            input_nodes[4][0] ^ isometries_nodes1[0][2]\n",
    "            \n",
    "            input_nodes[2][0] ^ isometries_nodes1[1][0]\n",
    "            input_nodes[3][0] ^ isometries_nodes1[1][1]\n",
    "            input_nodes[7][0] ^ isometries_nodes1[1][3]\n",
    "            \n",
    "            input_nodes[8][0] ^ isometries_nodes1[2][0]\n",
    "            input_nodes[12][0] ^ isometries_nodes1[2][2]\n",
    "            input_nodes[13][0] ^ isometries_nodes1[2][3]\n",
    "            \n",
    "            input_nodes[11][0] ^ isometries_nodes1[3][1]\n",
    "            input_nodes[14][0] ^ isometries_nodes1[3][2]\n",
    "            input_nodes[15][0] ^ isometries_nodes1[3][3]\n",
    "            \n",
    "            \n",
    "            isometries_nodes1[0][4] ^ e_nodes2[0]\n",
    "            isometries_nodes1[1][4] ^ e_nodes2[1]\n",
    "            isometries_nodes1[2][4] ^ e_nodes2[2]\n",
    "            isometries_nodes1[3][4] ^ e_nodes2[3]\n",
    "\n",
    "            e_nodes2[4] ^ isometries_nodes2[0]\n",
    "            e_nodes2[5] ^ isometries_nodes2[1]\n",
    "            e_nodes2[6] ^ isometries_nodes2[2]\n",
    "            e_nodes2[7] ^ isometries_nodes2[3]\n",
    "\n",
    "                            \n",
    "            nodes = tn.reachable(isometries_nodes2)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            #result = (c @ b).tensor\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        output = tf.vectorized_map(lambda vec: f(vec, self.entanglers1, self.entanglers2,\n",
    "                                                 self.isometries1,  self.isometries2, self.bias, self.n_layers), inputs)\n",
    "        return tf.reshape(output, (-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "grid4dmera_4 (Grid4DMERA)    (None, 10)                1114538   \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,114,538\n",
      "Trainable params: 1,114,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "MERA_model = tf.keras.Sequential()\n",
    "MERA_model.add(tf.keras.Input(shape=(16,16,1)))\n",
    "MERA_model.add(Grid4DMERA(input_dim=8, bond_dims=2, output_dims=10, n_layers=2))\n",
    "#MERA_model.add(Dense(10, activation='softmax', name='fc'))\n",
    "MERA_model.add(tf.keras.layers.Softmax())\n",
    "#MERA_model.build(input_shape=(None, 8, 8, 1))\n",
    "MERA_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 41s 679us/sample - loss: 994391578719796.1250 - accuracy: 0.3690\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 37s 613us/sample - loss: 1156218155896476.0000 - accuracy: 0.4946\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 32s 531us/sample - loss: 1184583999826445.2500 - accuracy: 0.5523\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 32s 529us/sample - loss: 1142528263562164.2500 - accuracy: 0.5932\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 31s 522us/sample - loss: 1265903136769520.5000 - accuracy: 0.6189\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 31s 523us/sample - loss: 1074394818739140.5000 - accuracy: 0.6500\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 31s 522us/sample - loss: 1191770500883214.0000 - accuracy: 0.6661\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 32s 525us/sample - loss: 1168405917405177.5000 - accuracy: 0.6845\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 31s 525us/sample - loss: 1093723869244205.7500 - accuracy: 0.7040\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 31s 517us/sample - loss: 1200954224380984.2500 - accuracy: 0.7149\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 32s 526us/sample - loss: 1113658643183566.8750 - accuracy: 0.7300\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 32s 528us/sample - loss: 1064157194148538.0000 - accuracy: 0.7425\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 32s 528us/sample - loss: 1135249778998282.2500 - accuracy: 0.7508\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 31s 519us/sample - loss: 1087150461745347.5000 - accuracy: 0.7603\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 32s 526us/sample - loss: 1098875099382231.8750 - accuracy: 0.7745\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 31s 524us/sample - loss: 1083695190969452.6250 - accuracy: 0.7824\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 32s 530us/sample - loss: 1071519154972009.0000 - accuracy: 0.7923\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 32s 536us/sample - loss: 1056934956843226.5000 - accuracy: 0.7990\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 31s 524us/sample - loss: 1106411025928325.3750 - accuracy: 0.8009\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 32s 537us/sample - loss: 1194652300805249.5000 - accuracy: 0.8083\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 34s 568us/sample - loss: 927063980501753.3750 - accuracy: 0.8238\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 31s 520us/sample - loss: 1017338431157008.2500 - accuracy: 0.8241\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 32s 534us/sample - loss: 1154080078362190.2500 - accuracy: 0.8254\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 32s 533us/sample - loss: 1074437957753049.3750 - accuracy: 0.8330\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 31s 524us/sample - loss: 1037190850287413.8750 - accuracy: 0.8396\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 31s 520us/sample - loss: 993401836530145.7500 - accuracy: 0.8427\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 31s 519us/sample - loss: 1004158349210480.2500 - accuracy: 0.8502\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 31s 516us/sample - loss: 1071215689825377.7500 - accuracy: 0.8551\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 31s 518us/sample - loss: 997028811405699.8750 - accuracy: 0.8586\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 31s 522us/sample - loss: 1034562655056595.1250 - accuracy: 0.8638\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 34s 570us/sample - loss: 1015448808054237.8750 - accuracy: 0.8670\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 48s 800us/sample - loss: 1043581108866008.1250 - accuracy: 0.8687\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 51s 857us/sample - loss: 1089152750017507.1250 - accuracy: 0.8724\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 32s 529us/sample - loss: 934599945095776.3750 - accuracy: 0.8795\n",
      "Epoch 35/50\n",
      "12416/60000 [=====>........................] - ETA: 9:27 - loss: 861605075511428.2500 - accuracy: 0.8874 ETA: 8:26 - loss: 861605075511428.2500 - accuracy: 0. - ETA: 9:27 - loss: 861605075511428.2500 - accuracy: 0.8874"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TensorNetwork model\n",
    "MERA_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0001), metrics=['accuracy'])\n",
    "MERA_hist = MERA_model.fit(xx_train, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TN model\n",
    "MERA_model.evaluate(x=xx_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid4DMERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, bond_dims, output_dims, n_layers=None):\n",
    "        super(Grid4DMERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 1936), we factorize it into a tensor (, 11, 11, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        if n_layers is None:\n",
    "            n_layers = np.floor(np.log2(input_dim))\n",
    "        self.n_layers = n_layers\n",
    "        in_dims = 16\n",
    "        dims = input_dim\n",
    "        self.entanglers = []\n",
    "        self.isometries= []\n",
    "            \n",
    "        # isometries\n",
    "        self.isometries1 = [tf.Variable(tf.random.normal(shape=(in_dims, in_dims, in_dims, \n",
    "                                                                            in_dims, bond_dims)\n",
    "                                                                     , stddev=1.0/10000),\n",
    "                                            trainable=True) for j in range(4)]\n",
    "        \n",
    "        self.isometries2 = tf.Variable(tf.random.normal(shape=(bond_dims, bond_dims, bond_dims, \n",
    "                                                                            bond_dims, output_dims)\n",
    "                                                                     , stddev=1.0/1000),\n",
    "                                            trainable=True)\n",
    "\n",
    "        #print(self.final_mps.shape)\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(output_dims,)), name=\"bias\", trainable=True)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, isometries1, isometries2, bias_var, n_layers):\n",
    "            input_vv = []\n",
    "            #print(input_vec)\n",
    "            for i in range(4):\n",
    "                for ii in range(4):\n",
    "                    input_vv.append(tf.reshape(input_vec[i*4:i*4+4, ii*4:ii*4+4, 0], (1, 16)))\n",
    "            input_vec = tf.concat(input_vv, axis=0)\n",
    "            input_vec = tf.reshape(input_vec, (16, 16))\n",
    "            input_vec = tf.unstack(input_vec)\n",
    "            input_nodes = []\n",
    "            for e_iv in input_vec:\n",
    "                input_nodes.append(tn.Node(e_iv))\n",
    "            \n",
    "                \n",
    "                                     \n",
    "            isometries_nodes1 = []\n",
    "            for eiso in isometries1:\n",
    "                isometries_nodes1.append(tn.Node(eiso))\n",
    "            isometries_nodes2 = tn.Node(isometries2)\n",
    "            \n",
    "             \n",
    "            \n",
    "            input_nodes[0][0] ^ isometries_nodes1[0][0]\n",
    "            input_nodes[1][0] ^ isometries_nodes1[0][1]\n",
    "            input_nodes[4][0] ^ isometries_nodes1[0][2]\n",
    "            input_nodes[5][0] ^ isometries_nodes1[0][3]\n",
    "\n",
    "            input_nodes[2][0] ^ isometries_nodes1[1][0]\n",
    "            input_nodes[3][0] ^ isometries_nodes1[1][1]\n",
    "            input_nodes[6][0] ^ isometries_nodes1[1][2]\n",
    "            input_nodes[7][0] ^ isometries_nodes1[1][3]\n",
    "            \n",
    "            input_nodes[8][0] ^ isometries_nodes1[2][0]         \n",
    "            input_nodes[9][0] ^ isometries_nodes1[2][1]\n",
    "            input_nodes[12][0] ^ isometries_nodes1[2][2]\n",
    "            input_nodes[13][0] ^ isometries_nodes1[2][3]\n",
    "            \n",
    "            input_nodes[10][0] ^ isometries_nodes1[3][0]\n",
    "            input_nodes[11][0] ^ isometries_nodes1[3][1]\n",
    "            input_nodes[14][0] ^ isometries_nodes1[3][2]\n",
    "            input_nodes[15][0] ^ isometries_nodes1[3][3]\n",
    "            \n",
    "            \n",
    "            isometries_nodes1[0][4] ^ isometries_nodes2[0]\n",
    "            isometries_nodes1[1][4] ^ isometries_nodes2[1]\n",
    "            isometries_nodes1[2][4] ^ isometries_nodes2[2]\n",
    "            isometries_nodes1[3][4] ^ isometries_nodes2[3]\n",
    "                            \n",
    "            nodes = tn.reachable(isometries_nodes2)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            #result = (c @ b).tensor\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        output = tf.vectorized_map(lambda vec: f(vec, self.isometries1,  self.isometries2, self.bias, self.n_layers), inputs)\n",
    "        return tf.reshape(output, (-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "two_dmera_4 (twoDMERA)       (None, 10)                170       \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 170\n",
      "Trainable params: 170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "MERA_model = tf.keras.Sequential()\n",
    "MERA_model.add(tf.keras.Input(shape=(28,28,1)))\n",
    "MERA_model.add(twoDMERA(input_dim=8, bond_dims=2, output_dims=10, n_layers=2))\n",
    "#MERA_model.add(Dense(10, activation='softmax', name='fc'))\n",
    "MERA_model.add(tf.keras.layers.Softmax())\n",
    "#MERA_model.build(input_shape=(None, 8, 8, 1))\n",
    "MERA_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TensorNetwork model\n",
    "MERA_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "MERA_hist = MERA_model.fit(xxx_train, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 20s 329us/sample - loss: 1.5148 - accuracy: 0.5273\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.1917 - accuracy: 0.6535\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.2757 - accuracy: 0.6247\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.1658 - accuracy: 0.6646\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 18s 296us/sample - loss: 1.1430 - accuracy: 0.6692\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.1260 - accuracy: 0.6725\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 18s 299us/sample - loss: 1.1309 - accuracy: 0.6745\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 18s 297us/sample - loss: 1.6934 - accuracy: 0.6329\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.1716 - accuracy: 0.6242\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 18s 297us/sample - loss: 0.9750 - accuracy: 0.7226\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 18s 304us/sample - loss: 3.1805 - accuracy: 0.6000\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 18s 299us/sample - loss: 1.6237 - accuracy: 0.4577\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 0.8443 - accuracy: 0.7593\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 18s 304us/sample - loss: 0.8651 - accuracy: 0.7609\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 18s 299us/sample - loss: 9.8902 - accuracy: 0.6293\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 18s 300us/sample - loss: 2.0853 - accuracy: 0.2585\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 18s 299us/sample - loss: 1.1962 - accuracy: 0.6263\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 0.7468 - accuracy: 0.7922\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 0.7404 - accuracy: 0.7968\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 18s 297us/sample - loss: 0.9587 - accuracy: 0.7413\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 18s 300us/sample - loss: 0.9951 - accuracy: 0.7347\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 18s 300us/sample - loss: 1.1901 - accuracy: 0.6989\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 0.9499 - accuracy: 0.7377\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 18s 301us/sample - loss: 0.9847 - accuracy: 0.7385\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 18s 299us/sample - loss: 1.0225 - accuracy: 0.7301\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 18s 299us/sample - loss: 1.0090 - accuracy: 0.7316\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.0089 - accuracy: 0.7304\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.0144 - accuracy: 0.7330\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.0457 - accuracy: 0.7273\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.0592 - accuracy: 0.7242\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 18s 299us/sample - loss: 0.9281 - accuracy: 0.7501\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 18s 300us/sample - loss: 1.0161 - accuracy: 0.7337\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 18s 302us/sample - loss: 1.4309 - accuracy: 0.6248\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 18s 301us/sample - loss: 0.9540 - accuracy: 0.7438\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 18s 304us/sample - loss: 10.1462 - accuracy: 0.2185\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 18s 296us/sample - loss: 1.7385 - accuracy: 0.3948\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 18s 296us/sample - loss: 0.8616 - accuracy: 0.7527\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 18s 297us/sample - loss: 0.7607 - accuracy: 0.7854\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 18s 296us/sample - loss: 0.9554 - accuracy: 0.7405\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 1.4680 - accuracy: 0.6153\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 0.8808 - accuracy: 0.7553\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 18s 301us/sample - loss: 0.9967 - accuracy: 0.7283\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 18s 301us/sample - loss: 1.0112 - accuracy: 0.7300\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 18s 297us/sample - loss: 1.2142 - accuracy: 0.6735\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 18s 299us/sample - loss: 2.1015 - accuracy: 0.5701\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 0.7082 - accuracy: 0.8136\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 18s 297us/sample - loss: 0.9467 - accuracy: 0.7560\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 18s 299us/sample - loss: 0.9882 - accuracy: 0.7437\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 3.0498 - accuracy: 0.5648\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 18s 298us/sample - loss: 0.9864 - accuracy: 0.6881\n",
      "CPU times: user 14min 35s, sys: 6.47 s, total: 14min 42s\n",
      "Wall time: 14min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TensorNetwork model\n",
    "MERA_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "MERA_hist = MERA_model.fit(xx_train, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 233us/sample - loss: nan - accuracy: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, 0.098]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TN model\n",
    "MERA_model.evaluate(x=xx_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 247us/sample - loss: 0.6466 - accuracy: 0.8453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6465772484540939, 0.8453]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TN model\n",
    "MERA_model.evaluate(x=xx_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = hist.history['loss']\n",
    "#acc = hist.history['accuracy']\n",
    "tn_loss = tn_hist.history['loss']\n",
    "tn_acc = tn_hist.history['accuracy']\n",
    "\n",
    "#plt.plot(loss, label='traditional')\n",
    "plt.plot(tn_loss, label='tn')\n",
    "plt.title('Loss comparsion in training')\n",
    "plt.ylim(0, 0.15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plt.plot(acc, label='traditional')\n",
    "plt.plot(tn_acc, label='tn')\n",
    "plt.title('Accuracy comparsion in training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim, n_nodes):\n",
    "        super(MERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 784), we factorize it into a tensor (, 7, 7, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        self.dim = dim\n",
    "        self.a_var = tf.Variable(tf.random.normal(shape=(7, 7), stddev=1.0/dim), name=\"a\", trainable=True)\n",
    "        self.b_var = tf.Variable(tf.random.normal(shape=(7, 7, 7), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.c_var = tf.Variable(tf.random.normal(shape=(16, 16), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        \n",
    "        self.d_var = tf.Variable(tf.random.normal(shape=(7, 7, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.e_var = tf.Variable(tf.random.normal(shape=(7, 16, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.f_var = tf.Variable(tf.random.normal(shape=(10, 10, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(10)), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, a_var, b_var, c_var, d_var, e_var, f_var, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            input_vec = tf.reshape(input_vec, (7, 7, 16))\n",
    "        \n",
    "            # Now we create the network.\n",
    "            a = tn.Node(a_var)\n",
    "            b = tn.Node(b_var)\n",
    "            c = tn.Node(c_var)\n",
    "            d = tn.Node(d_var)\n",
    "            e = tn.Node(e_var)\n",
    "            f = tn.Node(f_var)\n",
    "            \n",
    "            x_node = tn.Node(input_vec)\n",
    "            a[0] ^ x_node[0]\n",
    "            b[0] ^ x_node[1]\n",
    "            c[0] ^ x_node[2]\n",
    "            a[1] ^ d[0]\n",
    "            b[1] ^ d[1]\n",
    "            b[2] ^ e[0]\n",
    "            c[1] ^ e[1]\n",
    "            d[2] ^ f[0]\n",
    "            e[2] ^ f[1]\n",
    "            \n",
    "\n",
    "            # The TN should now look like this\n",
    "            #          |\n",
    "            #          f\n",
    "            #       /    \\\n",
    "            #     d       e\n",
    "            #   /    \\ /    \\\n",
    "            #   a     b     c\n",
    "            #    \\   |     /\n",
    "            #        x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            # c1 = a @ x_node # avoid trace edge?\n",
    "            # c2 = c1 @ b\n",
    "            #c1 = a @ x_node\n",
    "            #c2 = c1 @ b\n",
    "            #c3 = c2 @ c\n",
    "            #result = c3.tensor\n",
    "            nodes = tn.reachable(f)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            \n",
    "            #result = (c @ b).tensor\n",
    "\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.a_var, self.b_var, self.c_var, \n",
    "                                                 self.d_var, self.e_var, self.f_var, self.bias), inputs)\n",
    "        return tf.reshape(result, (-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mera (MERA)                  multiple                  3268      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 3,268\n",
      "Trainable params: 3,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MERA_model = tf.keras.Sequential()\n",
    "#MERA_model.add(tf.keras.Input(shape=(8,8,1)))\n",
    "MERA_model.add(MERA(10,2))\n",
    "MERA_model.add(tf.keras.layers.Softmax())\n",
    "MERA_model.build(input_shape=(None, 28, 28, 1))\n",
    "MERA_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 2.2900 - accuracy: 0.1211\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 2.2666 - accuracy: 0.1448\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.2660 - accuracy: 0.1463\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 2.2660 - accuracy: 0.1477\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 2.2660 - accuracy: 0.1478\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 2.2660 - accuracy: 0.1468\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 2.2660 - accuracy: 0.1466\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 2.2660 - accuracy: 0.1466\n",
      "Epoch 9/20\n",
      "23264/60000 [==========>...................] - ETA: 2s - loss: 2.2657 - accuracy: 0.1443"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/hpchome/carin/fk43/anaconda3/envs/tfquant/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TensorNetwork model\n",
    "MERA_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "tn_hist = MERA_model.fit(xx_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERA(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dim, n_nodes):\n",
    "        super(MERA, self).__init__()\n",
    "        # Create the variables for the layer.\n",
    "        # In this case, the input tensor is (, 784), we factorize it into a tensor (, 7, 7, 16)\n",
    "        # first_dim: output shape?\n",
    "        # second_dim: connect with data tensor\n",
    "        # third_dim: inter-connect\n",
    "        self.dim = dim\n",
    "        self.a_var = tf.Variable(tf.random.normal(shape=(7, 7), stddev=1.0/dim), name=\"a\", trainable=True)\n",
    "        self.b_var = tf.Variable(tf.random.normal(shape=(7, 7, 7), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.c_var = tf.Variable(tf.random.normal(shape=(16, 16), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        \n",
    "        self.d_var = tf.Variable(tf.random.normal(shape=(7, 7, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.e_var = tf.Variable(tf.random.normal(shape=(7, 16, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "        self.f_var = tf.Variable(tf.random.normal(shape=(10, 10, 10), stddev=1.0/dim), name=\"b\", trainable=True)\n",
    "\n",
    "        self.bias = tf.Variable(tf.zeros(shape=(10)), name=\"bias\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the contraction.\n",
    "        # We break it out so we can parallelize a batch using tf.vectorized_map.\n",
    "        def f(input_vec, a_var, b_var, c_var, d_var, e_var, f_var, bias_var):\n",
    "            # Reshape to a matrix instead of a vector.\n",
    "            input_vec = tf.reshape(input_vec, (7, 7, 16))\n",
    "        \n",
    "            # Now we create the network.\n",
    "            a = tn.Node(a_var)\n",
    "            b = tn.Node(b_var)\n",
    "            c = tn.Node(c_var)\n",
    "            d = tn.Node(d_var)\n",
    "            e = tn.Node(e_var)\n",
    "            f = tn.Node(f_var)\n",
    "            \n",
    "            x_node = tn.Node(input_vec)\n",
    "            a[0] ^ x_node[0]\n",
    "            b[0] ^ x_node[1]\n",
    "            c[0] ^ x_node[2]\n",
    "            a[1] ^ d[0]\n",
    "            b[1] ^ d[1]\n",
    "            b[2] ^ e[0]\n",
    "            c[1] ^ e[1]\n",
    "            d[2] ^ f[0]\n",
    "            e[2] ^ f[1]\n",
    "            \n",
    "\n",
    "            # The TN should now look like this\n",
    "            #          |\n",
    "            #          f\n",
    "            #       /    \\\n",
    "            #     d       e\n",
    "            #   /    \\ /    \\\n",
    "            #   a     b     c\n",
    "            #    \\   |     /\n",
    "            #        x\n",
    "\n",
    "            # Now we begin the contraction.\n",
    "            # c1 = a @ x_node # avoid trace edge?\n",
    "            # c2 = c1 @ b\n",
    "            #c1 = a @ x_node\n",
    "            #c2 = c1 @ b\n",
    "            #c3 = c2 @ c\n",
    "            #result = c3.tensor\n",
    "            nodes = tn.reachable(f)\n",
    "            result = tn.contractors.greedy(nodes)\n",
    "            result = result.tensor\n",
    "            #print(result)\n",
    "            \n",
    "            #result = (c @ b).tensor\n",
    "\n",
    "            # Finally, add bias.\n",
    "            return result + bias_var\n",
    "\n",
    "        # To deal with a batch of items, we can use the tf.vectorized_map function.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "        result = tf.vectorized_map(lambda vec: f(vec, self.a_var, self.b_var, self.c_var, \n",
    "                                                 self.d_var, self.e_var, self.f_var, self.bias), inputs)\n",
    "        return tf.nn.relu(tf.reshape(result, (-1, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
